{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elainedias16/TCC/blob/main/Next_step_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ku2I-_TlrUyM",
        "outputId": "04f6f216-e8bc-47d4-bcf1-e168d55feba9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.40)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Architeture\n",
        "https://dugas.ch/artificial_curiosity/GPT_architecture.html\n",
        "\n",
        "https://keras.io/examples/generative/text_generation_with_miniature_gpt/\n",
        "\n",
        "https://huggingface.co/learn/nlp-course/chapter7/6"
      ],
      "metadata": {
        "id": "wHlD7QV0c7RL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoY2CAyQPeWF"
      },
      "source": [
        "## Masked Self-Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scale dot produt attetion:\n",
        "https://paperswithcode.com/method/scaled"
      ],
      "metadata": {
        "id": "YyczlNf4nndV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Head(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.query = nn.Linear(config.d_model, config.head_dim, bias=config.bias)\n",
        "    self.key = nn.Linear(config.d_model, config.head_dim, bias=config.bias)\n",
        "    self.value = nn.Linear(config.d_model, config.head_dim, bias=config.bias)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    q = self.query(x)\n",
        "    k = self.key(x)\n",
        "    v = self.value(x)\n",
        "    return q, k, v\n",
        "\n",
        "\n",
        "\n",
        "class MaskedSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.num_heads = config.num_heads\n",
        "        self.head_dim = config.head_dim\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.d_model = config.d_model\n",
        "        self.heads = nn.ModuleList([Head(config) for _ in range(config.num_heads)])\n",
        "        self.output_linear = nn.Linear(config.d_model, config.d_model)\n",
        "        assert self.head_dim * self.num_heads == self.d_model, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        B, T, C = x.size()\n",
        "\n",
        "        heads_output = []\n",
        "        for head in self.heads:\n",
        "            k, q, v = head(x)\n",
        "\n",
        "\n",
        "            # Scaled dot-product attention\n",
        "            scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "\n",
        "            if mask is not None:\n",
        "                scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "            attn_weights = F.softmax(scores, dim=-1)\n",
        "            attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "            head_output = torch.matmul(attn_weights, v)\n",
        "            heads_output.append(head_output)\n",
        "\n",
        "        # Concatenate all heads' output\n",
        "        concatenated_output = torch.cat(heads_output, dim=-1) # (B, T, d_model)\n",
        "        output = self.output_linear(concatenated_output) # Final linear layer\n",
        "\n",
        "        return output\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NbvfI4tFlQ_u"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNfea0_8oLZw"
      },
      "source": [
        "## Feed Forward Nerual Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QyoN_gBEzPM"
      },
      "source": [
        "output = input * W + **bias**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dVxd-AS5ok8v"
      },
      "outputs": [],
      "source": [
        "class FeedFoward(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(config.d_model, 4 * config.d_model, bias=config.bias)\n",
        "    self.activation = nn.ReLU()\n",
        "    self.linear2 = nn.Linear(config.d_model * 4,  config.d_model, bias=config.bias)\n",
        "    self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.linear1(x)\n",
        "    x = self.activation(x)\n",
        "    x = self.linear2(x)\n",
        "    x = self.dropout(x)\n",
        "    return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8FHuZBKLfMP"
      },
      "source": [
        "## Layer Norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hjA6NoboLhpe"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.norm = nn.LayerNorm(config.d_model, config.bias)\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.norm(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t72dRbgtM6Xy"
      },
      "source": [
        "## One Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4RAoljJTM8Al"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.ln_1 = LayerNorm(config)\n",
        "    self.masked_self_attention = MaskedSelfAttention(config)\n",
        "    self.ln_2 = LayerNorm(config)\n",
        "    self.feed_forward = FeedFoward(config)\n",
        "\n",
        "  # def forward(self, x, mask):\n",
        "  #   x = self.ln_1(x)\n",
        "  #   x = x + self.masked_self_attention(x, mask)\n",
        "  #   x = self.ln_2(x)\n",
        "  #   x = x + self.feed_forward(x)\n",
        "  #   return x\n",
        "  def forward(self, x):\n",
        "    x = self.ln_1(x)\n",
        "    x = x + self.masked_self_attention(x)\n",
        "    x = self.ln_2(x)\n",
        "    x = x + self.feed_forward(x)\n",
        "    return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uluREArFtEP-"
      },
      "source": [
        "## Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "id": "NFcZVsTDQnf5"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.word_token_embedding = nn.Embedding(config.vocab_size, config.d_model)\n",
        "    # self.position_embedding = nn.Embedding(config.block_size, config.d_model) #tava assim\n",
        "    self.position_embedding = nn.Embedding(config.max_length, config.d_model)\n",
        "    self.dropout = nn.Dropout(config.dropout)\n",
        "    self.blocks = nn.Sequential(*[Decoder(config) for _ in range(config.n_layer)])\n",
        "    self.ln = LayerNorm(config)\n",
        "    self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, input_ids, targets=None):\n",
        "    device = input_ids.device\n",
        "    B, T = input_ids.size()\n",
        "    # T = input_ids.size(1)\n",
        "    # mask = torch.ones(b, t, t, device=device)\n",
        "\n",
        "    # Positional e token embed\n",
        "    tok_emb = self.word_token_embedding(input_ids)\n",
        "    pos_emb = self.position_embedding(torch.arange(T, device=device))\n",
        "    x = self.dropout(tok_emb + pos_emb)\n",
        "    # Transformer blocks\n",
        "    x = self.blocks(x)\n",
        "    # Norm layer\n",
        "    x = self.ln(x)\n",
        "    # Final layer\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "    print(f'---------------logits-------------------: {logits}')\n",
        "\n",
        "    return logits, loss\n",
        "    # return logits, 0.01 #fazer a loss dps\n",
        "\n",
        "\n",
        "\n",
        "  def generate(self, input_ids, max_new_tokens):\n",
        "    new_tokens = []\n",
        "\n",
        "    for _ in range(0, max_new_tokens):\n",
        "      input_ids_cond = input_ids[:, -config.block_size:]\n",
        "      logits, loss = self.forward(input_ids_cond)\n",
        "      logits = logits[:, -1, :]\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "      input_ids_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "      new_tokens.append(input_ids_next)\n",
        "\n",
        "      input_ids = torch.cat((input_ids, input_ids_next), dim=1)\n",
        "\n",
        "    new_tokens = torch.cat(new_tokens, dim=1)\n",
        "    print(f\"len new tokens : {len(new_tokens)}\")\n",
        "    print(f\"--------------new tokens-------------: {new_tokens}\")\n",
        "    return new_tokens\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def output_text(tokenizer, ids_new_tokens):\n",
        "  out_text = tokenizer.decode(ids_new_tokens[0].tolist())\n",
        "  return out_text\n",
        "\n",
        "def print_all_sentence(promp):\n",
        "  out_text = output_text(tokenizer, new_tokens)\n",
        "  print(promp + out_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# logits, loss = model(input_ids)"
      ],
      "metadata": {
        "id": "0fInUsky1RYI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XzNJw5itApm"
      },
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": true,
        "id": "XVgYPtEGCfoY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class Config:\n",
        "    num_heads = 2\n",
        "    d_model = 8 #os vetores de entrada e saída terão dimensão 8\n",
        "    head_dim = 4 #cada cabeça tem dimensão 4\n",
        "    dropout = 0.1  #para evitar overfiting\n",
        "    bias = True\n",
        "    vocab_size = 50257  # len tokenizer\n",
        "    # hidden_size = 1024\n",
        "    max_length = 512\n",
        "    n_layer = 6\n",
        "    # block_size = 1024\n",
        "    # block_size = 32\n",
        "    block_size = 5\n",
        "\n",
        "    # hidden_size =  model.config.hidden_size,\n",
        "\n",
        "config = Config()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Paramters:\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "max_new_tokens = 5\n",
        "\n",
        "print(f\"Device is {device}\")\n",
        "print(f\"Max new tokens is {max_new_tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wY6vdfiFsCgS",
        "outputId": "02841784-407c-4bc2-dcca-ec9c347d910d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device is cpu\n",
            "Max new tokens is 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oIF44LztI_8"
      },
      "source": [
        "## Run model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "\n",
        "# Carregar o tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76SN0uOrZ2ph",
        "outputId": "1ec216d0-03bc-4456-ecbf-802c8067c65b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicialização da configuração e do modelo\n",
        "config = Config()\n",
        "model = Transformer(config)"
      ],
      "metadata": {
        "id": "vSp1QBnhbIRs"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "esWRi7zUdDaT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "4836b713-aa7a-4d34-c4eb-c611c570e99d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs ids\n",
            "tensor([[5211,  345,  588, 4771, 5633]])\n",
            "---------------logits-------------------: tensor([[[ 0.4573,  0.0993,  0.1494,  ...,  0.2900, -0.9119,  0.6620],\n",
            "         [-0.5898,  0.7022, -0.1504,  ...,  0.0707, -0.5243,  1.2423],\n",
            "         [-1.3917, -1.3462,  0.4715,  ...,  2.1473,  1.4047,  0.7388],\n",
            "         [-0.9162, -0.4383,  0.5053,  ...,  0.6828,  0.5929,  0.7857],\n",
            "         [ 1.1162,  0.0470,  0.5647,  ..., -0.8057, -0.9162,  0.2983]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n",
            "logits\n",
            "tensor([[[ 0.4573,  0.0993,  0.1494,  ...,  0.2900, -0.9119,  0.6620],\n",
            "         [-0.5898,  0.7022, -0.1504,  ...,  0.0707, -0.5243,  1.2423],\n",
            "         [-1.3917, -1.3462,  0.4715,  ...,  2.1473,  1.4047,  0.7388],\n",
            "         [-0.9162, -0.4383,  0.5053,  ...,  0.6828,  0.5929,  0.7857],\n",
            "         [ 1.1162,  0.0470,  0.5647,  ..., -0.8057, -0.9162,  0.2983]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n",
            "loss\n",
            "None\n",
            "---------------logits-------------------: tensor([[[ 0.5894,  0.0898, -0.0987,  ...,  0.3488, -0.8367,  0.4068],\n",
            "         [-1.8016,  0.3929,  0.0073,  ..., -0.4708,  0.7898,  2.1360],\n",
            "         [-0.7651, -1.2884,  0.0951,  ...,  2.4487,  0.9382,  0.5401],\n",
            "         [-0.3449, -0.0121, -0.0220,  ..., -0.5130,  0.1925,  0.7250],\n",
            "         [ 0.9076, -0.1566,  0.5414,  ..., -0.5192, -0.4584,  0.4269]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n",
            "---------------logits-------------------: tensor([[[-0.6243,  0.5071,  0.6274,  ..., -1.7439, -0.3550,  1.3413],\n",
            "         [-0.1478,  0.1705,  0.4184,  ..., -0.9082, -0.6138,  1.1208],\n",
            "         [-1.8629, -0.4151,  0.9037,  ...,  0.9775,  1.0668,  0.9947],\n",
            "         [-0.7358, -0.6075,  0.7621,  ...,  1.1322,  0.8849,  0.1568],\n",
            "         [-0.4448, -0.3525,  2.0359,  ..., -3.0500, -0.1627,  1.7327]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n",
            "---------------logits-------------------: tensor([[[ 0.0647, -0.6475,  1.0133,  ...,  0.3721, -0.4077,  0.4864],\n",
            "         [ 0.2104,  0.3981,  0.7252,  ..., -0.7927, -1.1209,  0.5072],\n",
            "         [-0.2171, -0.2743,  0.8327,  ..., -0.0071,  0.0338,  0.0386],\n",
            "         [-0.3668, -0.7274,  1.7182,  ...,  0.4093, -0.3618,  1.3819],\n",
            "         [ 0.5644, -1.6164,  2.3141,  ...,  0.2851, -0.4174,  1.9136]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n",
            "---------------logits-------------------: tensor([[[ 0.2319, -0.1626,  0.9948,  ..., -1.3182, -0.6526,  1.0459],\n",
            "         [-0.0482,  0.0984,  0.3990,  ..., -0.9614, -0.2417,  0.3479],\n",
            "         [-1.5959, -0.0476,  1.4521,  ..., -1.4762,  0.2363,  2.0004],\n",
            "         [-2.6581, -1.8237,  2.1471,  ...,  2.9431,  2.1248,  2.1594],\n",
            "         [-0.3423, -0.1862,  1.3100,  ..., -1.1170,  0.2896, -0.4537]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n",
            "---------------logits-------------------: tensor([[[ 2.7632e-01, -1.0618e+00,  5.5629e-01,  ...,  4.0523e-01,\n",
            "           1.1394e-01,  8.4049e-01],\n",
            "         [-2.2296e-01,  2.6630e-01,  7.9290e-01,  ..., -1.4893e+00,\n",
            "          -7.1070e-01,  8.4380e-01],\n",
            "         [-7.6220e-01, -1.4116e+00,  8.1665e-01,  ...,  2.2010e+00,\n",
            "           7.8324e-01,  1.4286e+00],\n",
            "         [-8.9249e-01, -2.5273e-02,  2.9238e-01,  ...,  1.4863e+00,\n",
            "           4.5050e-01, -9.0490e-01],\n",
            "         [-2.5228e-01, -1.1327e-03,  7.8778e-01,  ..., -8.6081e-01,\n",
            "          -3.7193e-01,  1.5418e+00]]], grad_fn=<UnsafeViewBackward0>)\n",
            "len new tokens : 1\n",
            "--------------new tokens-------------: tensor([[26787,  4174, 16704,  6889, 26728]])\n",
            "out text:  theoretically apply characteristic Make swings\n",
            "Do you like ice ? theoretically apply characteristic Make swings\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "\n",
        "\n",
        "# Tokenizar a entrada\n",
        "prompt = \"Do you like ice ?\"\n",
        "input_ids = tokenizer(prompt, return_tensors='pt')['input_ids']\n",
        "\n",
        "print(\"Inputs ids\")\n",
        "print(input_ids)\n",
        "\n",
        "\n",
        "# Garantir que os IDs dos tokens estejam dentro do vocabulário do modelo\n",
        "unknown_token_id = tokenizer.unk_token_id\n",
        "input_ids[input_ids >= config.vocab_size] = unknown_token_id\n",
        "\n",
        "# Mover o modelo para o dispositivo\n",
        "model = model.to(device)\n",
        "\n",
        "# Mover input_ids para o dispositivo\n",
        "input_ids = input_ids.to(device)\n",
        "logits, loss = model(input_ids, targets=None)\n",
        "print(\"logits\")\n",
        "print(logits)\n",
        "print(\"loss\")\n",
        "print(loss)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Generate new tokens\n",
        "new_tokens = model.generate(input_ids, max_new_tokens) # Use the generate method to get new tokens\n",
        "\n",
        "# Print the generated text\n",
        "out_text = output_text(tokenizer, new_tokens) # Pass the new tokens to print_output\n",
        "print(f\"out text: {out_text}\")\n",
        "print_all_sentence(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HPEEv5k7c50o"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "eoY2CAyQPeWF",
        "pNfea0_8oLZw",
        "e8FHuZBKLfMP"
      ],
      "provenance": [],
      "mount_file_id": "1P40xsKBfsl3KKataojOyf-0VioRb8Syf",
      "authorship_tag": "ABX9TyNxWLJIQ5HVs4pvKgLV7Xk0",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}