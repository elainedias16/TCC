{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "eoY2CAyQPeWF",
        "pNfea0_8oLZw",
        "e8FHuZBKLfMP"
      ],
      "authorship_tag": "ABX9TyOd+ShF1divZhsd+cmGNbZi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elainedias16/TCC/blob/main/Lab3_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ku2I-_TlrUyM",
        "outputId": "4b15c6f9-34d6-4af1-d8d3-e5ec2b4bf99e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Masked Self-Attention"
      ],
      "metadata": {
        "id": "eoY2CAyQPeWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "\n",
        "class MaskedSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.num_heads = config.num_heads\n",
        "        self.d_model = config.d_model\n",
        "        self.head_dim = config.head_dim\n",
        "        self.dropout = config.dropout\n",
        "        self.bias = config.bias\n",
        "        assert self.head_dim * self.num_heads == self.d_model, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        # Projeções key, query, value para todas as cabeças, mas em um batch\n",
        "        self.qkv_linear = nn.Linear(config.d_model, config.d_model * 3, bias=self.bias)\n",
        "        print(f\"self.qvk_linear {self.qkv_linear}\")\n",
        "\n",
        "        # Saída\n",
        "        self.out_linear = nn.Linear(config.d_model, config.d_model, bias=self.bias)\n",
        "        print(f\"self.out_linear {self.out_linear}\")\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_length, _ = x.size()\n",
        "        print(f\"batch_size, seq_length, _ {batch_size, seq_length, _}\")\n",
        "        qkv = self.qkv_linear(x).reshape(batch_size, seq_length, self.num_heads, 3 * self.head_dim)\n",
        "        print(f\"qvk : {self.qkv_linear(x)}\")\n",
        "\n",
        "        qkv = qkv.permute(2, 0, 1, 3).chunk(3, dim=-1)  # (num_heads, batch_size, seq_length, head_dim)\n",
        "\n",
        "        queries, keys, values = qkv\n",
        "        attention_scores = torch.matmul(queries, keys.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "\n",
        "        if mask is not None:\n",
        "            attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        attention_weights = nn.functional.softmax(attention_scores, dim=-1)\n",
        "        attention_output = torch.matmul(attention_weights, values)\n",
        "        attention_output = attention_output.permute(1, 2, 0, 3).reshape(batch_size, seq_length, self.d_model)\n",
        "        return self.out_linear(attention_output)\n"
      ],
      "metadata": {
        "id": "ilHaqt2YXMT-",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feed Forward Nerual Network"
      ],
      "metadata": {
        "id": "pNfea0_8oLZw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "output = input * W + **bias**"
      ],
      "metadata": {
        "id": "8QyoN_gBEzPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedFoward(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(config.d_model, 4 * config.d_model, bias=config.bias)\n",
        "    self.activation = nn.ReLU()\n",
        "    self.linear2 = nn.Linear(config.d_model * 4,  config.d_model, bias=config.bias)\n",
        "    self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.linear1(x)\n",
        "    x = self.activation(x)\n",
        "    x = self.linear2(x)\n",
        "    x = self.dropout(x)\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "dVxd-AS5ok8v"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Layer Norm"
      ],
      "metadata": {
        "id": "e8FHuZBKLfMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.norm = nn.LayerNorm(config.d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.norm(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "hjA6NoboLhpe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One Decoder"
      ],
      "metadata": {
        "id": "t72dRbgtM6Xy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.ln_1 = LayerNorm(config)\n",
        "    self.masked_self_attention = MaskedSelfAttention(config)\n",
        "    self.ln_2 = LayerNorm(config)\n",
        "    self.feed_forward = FeedFoward(config)\n",
        "\n",
        "  # def forward(self, x, mask):\n",
        "  #   x = self.ln_1(x)\n",
        "  #   x = x + self.masked_self_attention(x, mask)\n",
        "  #   x = self.ln_2(x)\n",
        "  #   x = x + self.feed_forward(x)\n",
        "  #   return x\n",
        "  def forward(self, x):\n",
        "    x = self.ln_1(x)\n",
        "    x = x + self.masked_self_attention(x)\n",
        "    x = self.ln_2(x)\n",
        "    x = x + self.feed_forward(x)\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "4RAoljJTM8Al"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "\n",
        "    # self.transformer = nn.Transformer(config.num_heads, config.d_model, config.head_dim, config.dropout)\n",
        "\n",
        "    # Define o transformer como um dicionário de módulos\n",
        "    self.transformer = nn.ModuleDict(dict(\n",
        "        # Embedding para tokens (word embeddings)\n",
        "        wte = nn.Embedding(config.vocab_size, config.d_model),\n",
        "        # Embedding para posições (position embeddings)\n",
        "        wpe = nn.Embedding(config.block_size, config.d_model),\n",
        "        # Camada de dropout para regularização\n",
        "        drop = nn.Dropout(config.dropout),\n",
        "        # Lista de blocos de transformadores (camadas)\n",
        "        h = nn.ModuleList([Decoder(config) for _ in range(config.n_layer)]),\n",
        "        # Normalização final da camada\n",
        "        ln_f = LayerNorm(config),\n",
        "    ))\n",
        "    # Cabeçalho de linguagem, mapeando as saídas dos embeddings para o tamanho do vocabulário\n",
        "    self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "\n",
        "\n",
        "  def forward(self, input_ids):\n",
        "      device = input_ids.device\n",
        "      b, t = input_ids.size()\n",
        "      # mask = torch.ones(b, t, t, device=device)\n",
        "      # Create position ids\n",
        "      position_ids = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "      # Token and Position Embeddings\n",
        "      tok_emb = self.transformer.wte(input_ids)\n",
        "      pos_emb = self.transformer.wpe(position_ids)\n",
        "      x = self.transformer.drop(tok_emb + pos_emb)\n",
        "\n",
        "      # Transformer blocks\n",
        "      for block in self.transformer.h:\n",
        "          x = block(x)\n",
        "\n",
        "      # Final LayerNorm\n",
        "      x = self.transformer.ln_f(x)\n",
        "\n",
        "      # Output layer\n",
        "      logits = self.lm_head(x)\n",
        "\n",
        "      return logits\n",
        "\n",
        "\n",
        "config = Config()\n",
        "model = Transformer(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NFcZVsTDQnf5",
        "outputId": "3a43629b-8ba6-4fca-e5d4-ed240cdac4d5"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "self.qvk_linear Linear(in_features=8, out_features=24, bias=True)\n",
            "self.out_linear Linear(in_features=8, out_features=8, bias=True)\n",
            "self.qvk_linear Linear(in_features=8, out_features=24, bias=True)\n",
            "self.out_linear Linear(in_features=8, out_features=8, bias=True)\n",
            "self.qvk_linear Linear(in_features=8, out_features=24, bias=True)\n",
            "self.out_linear Linear(in_features=8, out_features=8, bias=True)\n",
            "self.qvk_linear Linear(in_features=8, out_features=24, bias=True)\n",
            "self.out_linear Linear(in_features=8, out_features=8, bias=True)\n",
            "self.qvk_linear Linear(in_features=8, out_features=24, bias=True)\n",
            "self.out_linear Linear(in_features=8, out_features=8, bias=True)\n",
            "self.qvk_linear Linear(in_features=8, out_features=24, bias=True)\n",
            "self.out_linear Linear(in_features=8, out_features=8, bias=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "\n",
        "class Config:\n",
        "    num_heads = 2\n",
        "    d_model = 8 #os vetores de entrada e saída terão dimensão 8\n",
        "    head_dim = 4 #cada cabeça tem dimensão 4\n",
        "    dropout = 0.1  #para evitar overfiting\n",
        "    bias = True\n",
        "    vocab_size = 30522\n",
        "    # hidden_size = 1024\n",
        "    max_length = 512\n",
        "    n_layer = 6\n",
        "    block_size = 1024\n",
        "    hidden_size =  model.config.hidden_size,\n",
        "\n",
        "config = Config()"
      ],
      "metadata": {
        "id": "XVgYPtEGCfoY",
        "collapsed": true
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "## DEU CERTO AQUII!!!!!!!\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# Tokenize input\n",
        "text = \"Hello princess\"\n",
        "input_ids = tokenizer(text, return_tensors='pt')['input_ids']\n",
        "\n",
        "# Forward pass\n",
        "with torch.no_grad():\n",
        "    logits = model(input_ids)\n",
        "\n",
        "\n",
        "\n",
        "print(type(logits))\n",
        "\n",
        "\n",
        "last_logit = logits[:, -1, :]\n",
        "print(f\"last_logit.shape: {last_logit.shape}\")\n",
        "\n",
        "predicted_next_token = torch.argmax(last_logit, dim=-1)\n",
        "\n",
        "predicted_token = tokenizer.decode(predicted_next_token.item())\n",
        "print(f\"Predicted next token: {predicted_token}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ss4pHTXmVLU",
        "outputId": "a025a68c-a7db-4c33-97c3-fd061aaf2ce9"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch_size, seq_length, _ (1, 2, 8)\n",
            "qvk : tensor([[[ 0.5060,  0.4781,  0.4893,  1.5728, -0.7622,  2.1378, -1.8663,\n",
            "          -0.4782, -0.5160, -1.5239, -0.9763, -0.9283, -1.1014,  0.9575,\n",
            "          -0.1674,  0.1979, -0.7322,  1.3459, -0.6323,  0.8999,  1.1452,\n",
            "          -1.1245,  0.2237,  1.1780],\n",
            "         [-0.7900,  0.3362,  0.6025, -1.4561,  2.4173,  1.0539,  0.2853,\n",
            "           0.1226,  0.3728, -0.9357,  0.5988, -0.1426,  0.9194,  0.1047,\n",
            "           1.1211,  1.6295, -1.1366, -0.4305, -0.3401, -0.2441, -1.1391,\n",
            "           0.7129, -1.7504, -0.9633]]])\n",
            "batch_size, seq_length, _ (1, 2, 8)\n",
            "qvk : tensor([[[-0.6294,  0.5671, -1.6691,  0.3663, -0.2499, -0.4849, -1.5110,\n",
            "          -1.0129,  0.6557, -1.5981, -1.4339,  1.0336,  1.8156,  0.7709,\n",
            "           0.4894,  0.9230, -0.9579, -1.5182,  1.3253,  1.3233,  1.0027,\n",
            "           0.6077,  1.5313,  0.8146],\n",
            "         [ 0.1418, -1.5399,  0.1373,  2.1984,  0.8395, -2.7964,  0.4247,\n",
            "          -1.0329,  2.1019,  0.8709,  0.9685,  1.8942,  1.1725, -0.2747,\n",
            "           2.2337, -0.6671, -1.9348,  0.0509,  1.4021, -0.2484,  1.2903,\n",
            "          -0.8676,  0.8294, -0.0367]]])\n",
            "batch_size, seq_length, _ (1, 2, 8)\n",
            "qvk : tensor([[[-0.7105,  1.2645, -0.0440, -0.6223, -1.9440,  0.8064, -0.0210,\n",
            "           0.5832,  0.9634, -0.6590, -0.1620, -0.9575, -0.1397, -0.6551,\n",
            "          -1.4464,  0.7308, -0.0277, -0.4048, -0.5969,  0.3595, -0.7908,\n",
            "           0.2305,  1.3175,  0.8378],\n",
            "         [ 0.8255,  0.2712,  1.8625, -0.9398, -0.3303,  0.3586,  0.3315,\n",
            "          -0.5386,  0.7008, -0.3322, -0.6449, -1.5562, -0.4210, -1.6762,\n",
            "          -0.0814, -1.5116,  0.9410,  0.6650,  0.4608, -1.1254, -0.4596,\n",
            "          -1.5438,  0.1847,  0.0584]]])\n",
            "batch_size, seq_length, _ (1, 2, 8)\n",
            "qvk : tensor([[[-0.2223, -0.2868, -0.9031, -1.3404, -0.3019, -0.5119,  0.1714,\n",
            "          -0.9873,  2.0618, -1.3487, -1.1299,  0.3650, -1.1360, -0.6049,\n",
            "          -0.1346, -0.9453, -2.0536,  0.1608,  1.0068, -0.2695,  1.0768,\n",
            "           0.2960, -0.1801, -0.2364],\n",
            "         [ 0.3443, -0.1926, -0.3018,  0.1595, -1.2687, -0.5161,  0.5929,\n",
            "           0.5203,  1.1964,  0.0299, -0.5334,  0.4382, -0.3519,  0.4249,\n",
            "          -0.2478,  1.5118,  0.0291,  1.1395, -0.8089,  0.8948,  1.1736,\n",
            "          -0.0136,  0.3849, -0.1327]]])\n",
            "batch_size, seq_length, _ (1, 2, 8)\n",
            "qvk : tensor([[[-0.8717, -0.8075,  1.3560,  0.5214, -1.7407, -1.0914,  1.3470,\n",
            "           1.4539,  1.1348,  1.9864, -1.3967,  0.6607,  0.7164, -0.8954,\n",
            "          -0.8740, -1.1686,  1.9616, -0.9471,  2.1411, -2.0872,  0.6761,\n",
            "           1.5817,  1.0404,  1.9703],\n",
            "         [-1.4410, -0.4159,  2.0595, -1.0372,  0.2544, -0.7548,  0.3895,\n",
            "          -0.6376,  1.6423,  1.1051,  0.5886,  1.7411,  1.5599, -1.7412,\n",
            "           0.1068, -1.5408, -0.4728,  0.6098,  0.1547,  0.1796,  0.4899,\n",
            "           0.4056,  0.3258,  1.3238]]])\n",
            "batch_size, seq_length, _ (1, 2, 8)\n",
            "qvk : tensor([[[-0.2553, -1.7978,  0.6514,  0.5352,  0.8735, -1.5665, -0.3133,\n",
            "           0.0161,  0.7439, -0.4501, -0.8684, -0.2769,  0.6503, -0.3804,\n",
            "          -1.1176, -0.1264, -1.2627, -0.0417,  0.9675, -0.5930, -0.2729,\n",
            "           0.2272,  1.1367, -0.6735],\n",
            "         [-2.1279, -1.1590, -2.4723, -1.3046,  0.4521, -1.1947, -0.8257,\n",
            "           1.0028, -0.5874, -0.5127, -1.0690, -1.4748, -0.8202,  1.0094,\n",
            "          -0.4478,  0.4925, -2.7816,  0.5383,  1.2085, -1.1791,  2.6572,\n",
            "           0.3381, -1.1843, -1.3125]]])\n",
            "<class 'torch.Tensor'>\n",
            "last_logit.shape: torch.Size([1, 30522])\n",
            "Predicted next token: cry\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize input\n",
        "text = \"The quick brown fox jumps\"\n",
        "input_ids = tokenizer(text, return_tensors='pt')['input_ids']\n",
        "\n",
        "# Forward pass\n",
        "model_output = model(input_ids)\n",
        "\n",
        "# Access the last hidden states of the last token\n",
        "# 'last_hidden_state' is the attribute name for the last hidden states in the model output\n",
        "last_logit = model_output.last_hidden_state[:, -1, :]\n",
        "print(f\"last_logit {last_logit}\")\n",
        "\n",
        "\n",
        "\n",
        "# Use torch.argmax para obter o índice do token com maior probabilidade\n",
        "predicted_next_token = torch.argmax(last_logit, dim=-1)\n",
        "\n",
        "\n",
        "predicted_token = tokenizer.decode(predicted_next_token.item())\n",
        "print(f\"Predicted next token: {predicted_token}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "QicSjURHjJI4",
        "outputId": "bb41fc32-609a-48f2-fd5c-2b140cbb1aab"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "last_logit tensor([[ 7.9428e-01, -1.0319e-01, -6.8416e-01,  5.5738e-01, -2.9287e-01,\n",
            "         -4.4239e-01,  2.2744e-01, -4.2560e-01,  4.1616e-01,  1.1188e-01,\n",
            "          4.6790e-01, -1.6452e-02,  2.5317e-01, -1.2033e-01, -7.6985e-01,\n",
            "         -5.7800e-01,  2.0851e-01,  7.7365e-02,  2.3898e-01,  2.1028e-01,\n",
            "          6.9469e-01, -8.0288e-02,  7.3058e-01,  4.6977e-01,  3.7426e-01,\n",
            "          2.7144e-01, -5.0824e-01, -1.9135e-01, -5.7742e-01, -4.6510e-01,\n",
            "         -4.8771e-01, -7.5875e-01,  6.5481e-02,  4.9138e-01,  4.2365e-01,\n",
            "          5.3647e-02,  2.6042e-01, -2.0944e-01, -5.3349e-01, -4.7316e-01,\n",
            "         -4.9004e-01,  1.3583e-01, -1.8085e-01,  2.8800e-01,  1.1426e-01,\n",
            "         -5.6994e-01,  6.8768e-01,  2.8650e-01,  5.7767e-02,  5.6858e-01,\n",
            "         -1.8224e-01,  3.3356e-01,  7.3571e-02,  1.5575e-01,  2.4582e-01,\n",
            "          2.5446e-01,  4.2865e-01, -4.7273e-01,  3.0086e-01, -5.4719e-02,\n",
            "          1.7036e-01,  4.3556e-01, -5.2030e-01, -1.7854e-01,  3.5339e-01,\n",
            "          2.3159e-01,  3.0863e-02, -7.7831e-02, -6.0327e-01, -3.9768e-01,\n",
            "         -1.4854e-01, -9.3788e-01,  6.8220e-01,  1.9177e-01,  1.7927e-01,\n",
            "          4.2413e-01, -6.4995e-01,  6.1249e-01,  3.6712e-01,  5.2219e-01,\n",
            "          2.7462e-01,  6.0583e-02,  1.7931e-01,  6.5449e-01,  3.0593e-01,\n",
            "         -2.2171e-02, -4.2509e-01,  1.1205e-01, -2.3904e-01, -2.3197e-01,\n",
            "          4.2562e-01,  1.4592e-01,  1.2686e-01, -1.8602e-01, -7.5078e-02,\n",
            "          3.8597e-01,  1.2866e-02, -7.6181e-02, -2.2458e-01,  5.8562e-02,\n",
            "          1.4610e-01, -1.9905e-01, -2.0980e-01,  1.0540e+00, -7.9103e-02,\n",
            "         -1.1701e-01, -2.4091e-03,  3.2609e-01,  3.2341e-01,  1.1115e+00,\n",
            "          7.6172e-01,  1.0542e-01,  2.2156e-01, -2.1951e-01, -4.2159e-01,\n",
            "         -2.7199e-01,  2.1147e-01,  1.2734e-01,  2.5080e-01,  1.5011e-01,\n",
            "         -4.9997e-01, -7.8934e-01,  4.1937e-01,  1.4310e+00, -1.3349e-01,\n",
            "         -4.9918e-02,  1.3262e-01, -7.7668e-01,  4.2722e-02, -7.2984e-01,\n",
            "         -6.7844e-01,  8.2489e-01,  5.5867e-01,  4.8183e-01, -1.7771e-02,\n",
            "          4.4694e-01, -1.0571e-01,  1.5337e-01, -8.5097e-01,  2.4900e-01,\n",
            "         -2.4423e-01,  7.7268e-01,  4.0271e-01, -8.1875e-01,  2.0207e-01,\n",
            "          4.9440e-01,  7.7652e-01,  7.2251e-02,  2.8074e-01, -3.9975e-01,\n",
            "          8.8890e-01, -4.0746e-01, -2.2602e-01,  6.7441e-02, -5.4825e-01,\n",
            "         -9.2186e-02, -3.6925e-01, -1.0953e-01,  7.6195e-01,  8.4950e-01,\n",
            "          1.9194e-01,  2.6044e-01,  3.8511e-01,  4.2460e-02, -6.2458e-01,\n",
            "         -9.4298e-02, -8.7729e-01, -1.5706e-02,  1.1737e-01,  5.2046e-01,\n",
            "         -6.9331e-01, -6.8758e-02, -5.7153e-02,  2.6399e-01, -2.4833e-01,\n",
            "          2.8067e-01, -6.6141e-02, -3.1797e-01, -8.0064e-02, -7.9482e-01,\n",
            "         -8.3887e+00, -1.7861e-01,  3.2432e-02,  6.8562e-02,  2.5191e-01,\n",
            "         -4.3724e-01, -5.6078e-01, -2.7968e-01, -1.7126e-01, -1.1207e+00,\n",
            "         -1.9089e-01, -9.0166e-02, -7.4313e-01,  5.8227e-01,  6.5883e-01,\n",
            "         -2.1344e-01, -4.4980e-02,  7.3534e-02, -5.5639e-01,  1.6085e-01,\n",
            "          2.0565e-01, -1.2183e-01,  7.2560e-02,  5.5919e-01, -2.7931e-01,\n",
            "         -1.6951e+00,  1.6544e-01, -2.0652e-01,  1.3718e-01,  6.9335e-02,\n",
            "         -1.1389e+00,  1.1448e-02, -2.9679e-01, -1.0434e-01, -3.8476e-02,\n",
            "         -2.9563e-01, -8.1206e-03, -5.2614e-01, -8.6724e-01, -2.7747e-01,\n",
            "         -7.3676e-01, -3.9781e-01, -3.2451e-01, -7.7259e-02,  7.6310e-01,\n",
            "         -1.5143e+00,  4.0581e-01,  8.2685e-01,  5.7061e-01,  3.2122e-01,\n",
            "          1.8916e-01, -7.9244e-02,  6.7275e-01,  2.9612e-01, -8.8072e-02,\n",
            "         -7.9782e-02, -3.5025e-02, -1.6509e-01, -2.4355e-01, -7.0080e-01,\n",
            "          1.2924e-01,  3.3069e-01,  1.9425e-01, -7.5233e-02, -4.0896e-01,\n",
            "         -4.1111e-01,  2.0532e-02,  8.2056e-01,  7.4961e-01, -3.3115e-01,\n",
            "         -2.4202e-01, -4.7435e-01,  5.0140e-01, -6.9037e-01,  4.1251e-01,\n",
            "          5.9670e-01,  1.8674e-01, -3.1428e-02,  8.7635e-01, -5.1602e-01,\n",
            "          9.1993e-01,  6.8393e-01,  6.3905e-01,  8.2035e-03, -3.3528e-01,\n",
            "          8.1081e-01,  3.8163e-01,  1.5504e-01, -2.6854e-01,  1.9394e-01,\n",
            "          1.6856e-01,  3.0473e-02, -1.3829e-01,  8.4992e-01, -1.5796e-01,\n",
            "         -1.1404e+00,  4.6412e-01, -9.8275e-02,  4.6664e-02, -4.0930e-01,\n",
            "         -4.0186e-01,  2.2844e-01, -2.6111e-01,  4.9021e-01,  4.9863e-01,\n",
            "         -4.8078e-01, -8.5237e-01,  5.0845e-02,  1.2270e-01, -7.4380e-01,\n",
            "         -1.9119e-01, -4.3255e-02, -6.5227e-02,  2.9300e-01, -3.4372e-01,\n",
            "          1.7490e-01,  8.5449e-01, -7.3481e-02, -2.1164e-01, -5.9352e-02,\n",
            "          2.0486e-01, -4.3266e-01, -2.0411e-03, -2.6525e-02, -4.2101e-01,\n",
            "          1.1753e-01,  2.5066e-01,  6.6552e-02,  9.0150e-01, -1.7238e-03,\n",
            "          3.1525e-01, -7.6598e-01,  7.4015e-02, -1.8404e-01,  1.2812e-01,\n",
            "          4.1295e-01, -2.3381e-01, -4.2014e-01, -2.1719e-01, -6.9639e-01,\n",
            "          4.8911e-01,  1.0391e-02, -8.6823e-02,  3.8675e-01,  1.8834e-01,\n",
            "         -1.7033e-01, -7.2588e-01, -2.4267e-01, -4.2239e-02,  2.6645e-01,\n",
            "          2.7533e-01, -7.9416e-02,  8.0343e-01, -3.5151e-02, -6.1264e-01,\n",
            "         -3.4091e-01,  2.3562e-01, -3.0882e-02, -1.6657e-01, -6.8588e-01,\n",
            "         -5.9663e-01, -2.1559e-01,  3.1809e-01, -9.8444e-02, -1.6750e-01,\n",
            "          3.0413e-01,  2.8103e-01, -3.6789e-01,  1.7509e-01,  5.4758e-01,\n",
            "         -3.5388e-01, -5.7985e-01, -4.8434e-01, -1.9674e-01, -1.3046e-01,\n",
            "         -4.1864e-01,  6.5014e-03,  3.2210e-01,  6.4493e-01,  2.5993e-02,\n",
            "         -5.0646e-01,  6.0944e-02, -4.2950e-01, -7.3723e-01,  9.5535e-03,\n",
            "          8.0262e-02, -2.8055e-01, -4.3588e-01,  2.1549e-01, -2.6112e-01,\n",
            "          5.1010e-01,  2.6816e-01, -1.0506e-01, -2.7563e-02,  3.3487e-01,\n",
            "          6.3224e-01, -2.6530e-01,  1.5784e-01,  4.0021e-01, -4.6955e-01,\n",
            "         -4.7816e-01, -6.1001e-01, -2.5560e-01,  5.1222e-01, -4.9264e-01,\n",
            "          2.6463e-01,  4.5637e-01,  2.1217e-01,  3.8866e-01,  7.7935e-02,\n",
            "         -1.2364e-01,  2.3321e-01, -1.5729e-01, -3.0651e-01, -1.1518e-01,\n",
            "         -9.5720e-02,  3.1884e-01, -8.9588e-01,  3.9268e-01,  2.1701e-01,\n",
            "         -1.1233e-01,  2.5664e-01,  1.5860e-01,  2.0609e-01,  2.2642e-01,\n",
            "          1.6763e-01,  4.2945e-02,  2.2892e-01, -2.1197e-01, -1.2448e-01,\n",
            "          7.8551e-01, -2.3007e-01, -2.6491e-01,  4.7657e-01, -2.9056e-01,\n",
            "         -5.6858e-01, -3.1111e-01, -4.3559e-01, -1.2486e-02, -6.2461e-01,\n",
            "          4.6935e-01, -7.8433e-02, -2.1159e-01, -5.8985e-01,  1.9788e-01,\n",
            "          7.5588e-01, -2.3641e-01,  5.9704e-01,  1.7240e-01,  3.8000e-02,\n",
            "          3.8642e-01,  6.9328e-01, -1.4840e-01, -1.3419e-01,  5.0308e-02,\n",
            "         -1.7249e-01,  4.5403e-01, -2.7731e-01,  2.6164e-01,  1.9391e-02,\n",
            "         -1.0304e-01, -4.2849e-01, -2.4755e-01,  4.1443e-01,  3.1030e-01,\n",
            "          9.1923e-01,  7.4195e-01,  4.2294e-01,  2.8396e-01,  5.1436e-01,\n",
            "         -8.4522e-01, -5.9553e-01,  1.9560e-02,  5.7690e-01, -6.5287e-01,\n",
            "          1.5090e-01,  6.0340e-01,  4.0187e-03, -2.0492e-01,  3.2851e-01,\n",
            "          4.0411e-01, -9.7349e-01,  3.3016e-01, -3.0541e-01,  2.0597e-02,\n",
            "          2.2640e-01, -8.5841e-01,  5.7486e-01, -4.8099e-01,  1.3014e-01,\n",
            "          5.0253e-01,  3.7337e-02, -7.5228e-01, -9.9907e-01,  4.9335e-01,\n",
            "         -6.6372e-01, -3.0808e-01,  2.3105e-01,  6.0233e-01, -8.1125e-02,\n",
            "          6.3372e-01, -8.5437e-02, -3.8524e-01,  2.2556e-01,  9.8306e-02,\n",
            "         -5.3665e-01, -1.8575e-01, -9.5422e-03,  9.1915e-02, -1.0544e+00,\n",
            "         -2.2983e-01,  8.5571e-03,  2.9403e-01,  2.3540e-01,  5.0209e-02,\n",
            "         -6.3845e-01,  3.9021e-01,  5.4860e-02,  4.9685e-01, -3.9226e-01,\n",
            "         -6.7174e-01,  2.3622e-01, -3.8389e-01,  6.1281e-01,  2.9144e-01,\n",
            "         -1.5574e-01,  5.1394e-01, -6.5571e-05,  6.8537e-01,  3.8900e-01,\n",
            "         -4.2536e-01,  1.3216e-01, -2.8437e-01, -2.8450e-01, -7.7797e-02,\n",
            "          2.1318e-01, -3.7108e-01, -3.5470e-01,  1.6739e-02, -6.5750e-01,\n",
            "         -4.7925e-01, -2.9438e-02,  5.0065e-01,  3.9556e-01,  2.5986e-01,\n",
            "         -1.4612e-01,  1.0317e+00, -4.0857e-01, -1.0036e+00, -6.3718e-02,\n",
            "          9.2832e-02,  2.6632e-01,  6.0977e-02, -8.8229e-01, -6.8940e-01,\n",
            "          4.9402e-01, -4.8351e-01,  1.7514e-01,  4.2935e-01,  5.7193e-01,\n",
            "         -7.3812e-01,  1.6283e-01,  5.2334e-01,  5.1657e-01, -3.4125e-01,\n",
            "         -1.0611e-01, -4.9872e-02, -1.2978e-01, -2.6063e-01, -2.0368e-02,\n",
            "         -1.1880e-01, -2.8226e-01,  4.4588e-01, -3.5926e-01,  2.5535e-01,\n",
            "          1.4780e+00, -1.5755e-01, -7.8472e-01, -1.4141e-01, -5.8384e-02,\n",
            "          3.9574e-01,  6.0898e-02, -7.2676e-01, -5.7207e-01, -4.7125e-01,\n",
            "          4.6738e-04, -4.0560e-01, -3.2064e-01,  2.5589e-02, -3.9669e-02,\n",
            "          8.6764e-01, -5.0366e-01,  4.3417e-01, -8.1951e-02, -3.4190e-01,\n",
            "          1.1790e-01,  1.2462e-01,  2.3975e-01,  8.5945e-03,  4.3410e-01,\n",
            "         -1.5722e-01,  1.0146e-01, -8.9212e-01,  3.4621e-01,  5.8885e-01,\n",
            "         -4.5261e-01, -6.1331e-01, -1.8280e-01, -2.7550e-02,  2.9889e-01,\n",
            "         -4.2858e-01, -1.1388e-01, -1.4687e-01, -4.2293e-01,  2.2491e-01,\n",
            "          4.2169e-01, -1.9597e-01, -3.9985e-01,  1.0773e+00,  2.0306e-01,\n",
            "         -3.9737e-01,  4.6855e-03, -2.5896e-01,  2.1539e-01, -3.0860e-01,\n",
            "          4.6561e-01, -7.9340e-02,  7.2949e-02, -3.0905e-02,  6.9787e-01,\n",
            "          1.0433e-01, -2.6497e-02, -7.7390e-02, -2.4937e-01,  2.7418e-01,\n",
            "          2.6808e-02,  9.7396e-02,  2.3037e-01, -6.0851e-01,  2.6801e-01,\n",
            "          5.5902e-02,  5.0322e-01, -1.1261e+00, -9.7591e-02,  1.3587e-01,\n",
            "         -7.1087e-01,  2.8740e-01,  6.0342e-01,  3.1703e-01,  2.5573e-01,\n",
            "          2.7942e-01, -2.5364e-01,  2.2783e-01,  4.6668e-01, -4.1849e-01,\n",
            "         -2.7344e-01, -5.6700e-01,  2.7381e-02,  5.7720e-01,  8.3549e-01,\n",
            "         -1.2654e-01,  4.8171e-01,  1.9819e-01, -1.4746e-01,  1.3635e-01,\n",
            "          4.4460e-01,  1.4300e-01, -4.4075e-01,  1.7333e-01, -2.3234e-02,\n",
            "          4.6516e-02,  2.3427e-01,  2.3454e-01,  5.9297e-02,  1.4046e-01,\n",
            "         -4.9443e-01,  8.1898e-02, -4.0085e-01,  4.6152e-01, -1.4698e-01,\n",
            "         -1.0814e-01,  3.5554e-01, -6.3274e-01,  8.8904e-02,  3.9652e-01,\n",
            "          2.5460e-01,  3.2355e-01, -8.9104e-02, -9.1171e-02, -6.0439e-01,\n",
            "         -9.9959e-01, -6.3725e-01,  1.9953e-01,  8.9195e-01,  4.4859e-01,\n",
            "         -5.4653e-01, -6.4430e-01,  4.2121e-01,  9.9040e-01,  8.6298e-01,\n",
            "         -6.3550e-01, -8.0321e-01, -1.2397e-01,  5.5018e-01, -6.9498e-01,\n",
            "          5.2570e-01, -9.3909e-01,  2.3303e-01,  5.6001e-01, -1.3347e-01,\n",
            "         -3.3793e-01,  1.2651e-01,  3.9478e-01, -5.0360e-01, -2.0785e-01,\n",
            "         -4.1793e-01,  5.7686e-02, -1.8847e-02,  3.2328e-01, -1.0026e+00,\n",
            "         -8.2679e-02, -2.9026e-01,  6.7836e-01,  1.6082e-02,  8.4836e-01,\n",
            "          1.5819e-01,  4.7844e-01,  4.1035e-01,  8.0759e-01, -3.8140e-01,\n",
            "         -7.7633e-01,  2.8946e-01,  5.5303e-01, -3.7692e-01,  1.4736e+00,\n",
            "         -3.5269e-01,  4.8313e-01, -3.1089e-01, -9.1586e-01, -8.1933e-03,\n",
            "         -3.1938e+00,  6.6069e-02, -2.3744e-01,  3.5104e-02, -3.3398e-01,\n",
            "          3.9515e-01, -1.9742e-01, -3.7607e-01, -6.0552e-02, -4.0279e-01,\n",
            "          3.7116e-01,  2.8391e-02, -2.2830e-01,  4.1632e-01,  2.7308e-01,\n",
            "          6.1937e-01, -1.7683e-01, -2.0468e-01,  3.5262e-01,  1.3280e-01,\n",
            "          2.3629e-01,  2.2014e-01,  3.2769e-02, -6.2975e-01, -4.0067e-01,\n",
            "          1.8301e-01, -6.5025e-01, -5.6996e-01, -7.6214e-02, -3.6236e-02,\n",
            "          3.2363e-01,  4.1813e-02,  6.9789e-01, -2.1715e-01,  7.1673e-01,\n",
            "         -1.6462e-01,  5.8096e-02, -4.3121e-01, -9.2946e-02,  2.3096e-01,\n",
            "         -6.6190e-01, -7.2316e-01,  3.7449e-01, -7.0118e-01,  2.3617e-01,\n",
            "          5.1235e-02, -6.0759e-01, -2.2982e-01]], grad_fn=<SliceBackward0>)\n",
            "Predicted next token: [ u n u s e d 5 5 0 ]\n"
          ]
        }
      ]
    }
  ]
}