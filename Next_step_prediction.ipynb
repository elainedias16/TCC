{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elainedias16/TCC/blob/main/Next_step_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ku2I-_TlrUyM",
        "outputId": "7f3dab63-41a9-4eac-85aa-914dcb122d0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.40)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Architeture\n",
        "https://dugas.ch/artificial_curiosity/GPT_architecture.html\n",
        "\n",
        "https://keras.io/examples/generative/text_generation_with_miniature_gpt/\n",
        "\n",
        "https://huggingface.co/learn/nlp-course/chapter7/6\n",
        "\n",
        "https://debuggercafe.com/text-generation-with-transformers/0\n",
        "\n",
        "https://debuggercafe.com/text-generation-with-transformers/"
      ],
      "metadata": {
        "id": "wHlD7QV0c7RL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoY2CAyQPeWF"
      },
      "source": [
        "## Masked Self-Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scale dot produt attetion:\n",
        "https://paperswithcode.com/method/scaled"
      ],
      "metadata": {
        "id": "YyczlNf4nndV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Head(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.query = nn.Linear(config.d_model, config.head_dim, bias=config.bias)\n",
        "    self.key = nn.Linear(config.d_model, config.head_dim, bias=config.bias)\n",
        "    self.value = nn.Linear(config.d_model, config.head_dim, bias=config.bias)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    q = self.query(x)\n",
        "    k = self.key(x)\n",
        "    v = self.value(x)\n",
        "    return q, k, v\n",
        "\n",
        "\n",
        "\n",
        "class MaskedSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.num_heads = config.num_heads\n",
        "        self.head_dim = config.head_dim\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.d_model = config.d_model\n",
        "        self.heads = nn.ModuleList([Head(config) for _ in range(config.num_heads)])\n",
        "        self.output_linear = nn.Linear(config.d_model, config.d_model)\n",
        "        assert self.head_dim * self.num_heads == self.d_model, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # B, T, C = x.size()\n",
        "\n",
        "        heads_output = []\n",
        "        for head in self.heads:\n",
        "            k, q, v = head(x)\n",
        "\n",
        "\n",
        "            # Scaled dot-product attention\n",
        "            scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "\n",
        "            if mask is not None:\n",
        "                scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "            attn_weights = F.softmax(scores, dim=-1)\n",
        "            attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "            head_output = torch.matmul(attn_weights, v)\n",
        "            heads_output.append(head_output)\n",
        "\n",
        "\n",
        "        concatenated_output = torch.cat(heads_output, dim=-1)\n",
        "        output = self.output_linear(concatenated_output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NbvfI4tFlQ_u"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNfea0_8oLZw"
      },
      "source": [
        "## Feed Forward Nerual Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QyoN_gBEzPM"
      },
      "source": [
        "\n",
        "MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "dVxd-AS5ok8v"
      },
      "outputs": [],
      "source": [
        "class FeedFoward(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(config.d_model, 4 * config.d_model, bias=config.bias)\n",
        "    self.activation = nn.ReLU()\n",
        "    self.linear2 = nn.Linear(config.d_model * 4,  config.d_model, bias=config.bias)\n",
        "    self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.linear1(x)\n",
        "    x = self.activation(x)\n",
        "    x = self.linear2(x)\n",
        "    x = self.dropout(x)\n",
        "    return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8FHuZBKLfMP"
      },
      "source": [
        "## Layer Norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "hjA6NoboLhpe"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.norm = nn.LayerNorm(config.d_model, config.bias)\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.norm(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t72dRbgtM6Xy"
      },
      "source": [
        "## One Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "4RAoljJTM8Al"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.ln_1 = LayerNorm(config)\n",
        "    self.masked_self_attention = MaskedSelfAttention(config)\n",
        "    self.ln_2 = LayerNorm(config)\n",
        "    self.feed_forward = FeedFoward(config)\n",
        "\n",
        "  # def forward(self, x, mask):\n",
        "  #   x = self.ln_1(x)\n",
        "  #   x = x + self.masked_self_attention(x, mask)\n",
        "  #   x = self.ln_2(x)\n",
        "  #   x = x + self.feed_forward(x)\n",
        "  #   return x\n",
        "  def forward(self, x):\n",
        "    x = self.ln_1(x)\n",
        "    x = x + self.masked_self_attention(x)\n",
        "    x = self.ln_2(x)\n",
        "    x = x + self.feed_forward(x)\n",
        "    return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uluREArFtEP-"
      },
      "source": [
        "## Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://medium.com/@hunter-j-phillips/positional-encoding-7a93db4109e6#:~:text=class%20PositionalEncoding(nn,self.dropout(x)\n",
        "olhar posiitional"
      ],
      "metadata": {
        "id": "S9mAVQgzvNCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.word_token_embedding = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.position_embedding = nn.Embedding(config.max_length, config.d_model)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.blocks = nn.Sequential(*[Decoder(config) for _ in range(config.n_layer)])\n",
        "        self.ln = LayerNorm(config)\n",
        "        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, input_ids, targets=None):\n",
        "        device = input_ids.device\n",
        "        B, T = input_ids.size()\n",
        "\n",
        "        # Positional e token embed\n",
        "        tok_emb = self.word_token_embedding(input_ids)\n",
        "        pos_emb = self.position_embedding(torch.arange(T, device=device))\n",
        "        x = self.dropout(tok_emb + pos_emb)\n",
        "\n",
        "        # Transformer blocks\n",
        "        x = self.blocks(x)\n",
        "\n",
        "        # Norm layer\n",
        "        x = self.ln(x)\n",
        "\n",
        "        # Final layer\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            # Shift logits and targets to align\n",
        "            shift_logits = logits[:, :-1, :].contiguous()\n",
        "            shift_targets = targets[:, 1:].contiguous()\n",
        "            loss = F.cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_targets.view(-1))\n",
        "            return logits, loss\n",
        "\n",
        "        return logits, None\n",
        "\n",
        "    def generate(self, input_ids, max_new_tokens):\n",
        "        new_tokens = []\n",
        "\n",
        "        for _ in range(0, max_new_tokens):\n",
        "            input_ids_cond = input_ids[:, -self.config.block_size:]\n",
        "            logits, _ = self.forward(input_ids_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            input_ids_next = torch.multinomial(probs, num_samples=1)\n",
        "            new_tokens.append(input_ids_next)\n",
        "            input_ids = torch.cat((input_ids, input_ids_next), dim=1)\n",
        "\n",
        "        new_tokens = torch.cat(new_tokens, dim=1)\n",
        "        return new_tokens\n"
      ],
      "metadata": {
        "id": "ftxqAuuVC_V7"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XzNJw5itApm"
      },
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "collapsed": true,
        "id": "XVgYPtEGCfoY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "#Paramters:\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "max_new_tokens = 50\n",
        "epochs = 100\n",
        "learning_rate = 0.001\n",
        "batch_size = 8\n",
        "SEQUENCE_LENGTH = 64\n",
        "\n",
        "\n",
        "class Config:\n",
        "    num_heads = 2\n",
        "    d_model = 8 #os vetores de entrada e saída terão dimensão 8\n",
        "    head_dim = 4 #cada cabeça tem dimensão 4\n",
        "    dropout = 0.1  #para evitar overfiting\n",
        "    bias = True\n",
        "    vocab_size = 50257  # len tokenizer\n",
        "    # hidden_size = 1024\n",
        "    max_length = 512\n",
        "    n_layer = 6\n",
        "    # block_size = 1024\n",
        "    # block_size = 32\n",
        "    block_size = SEQUENCE_LENGTH\n",
        "\n",
        "\n",
        "config = Config()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "TMRf3-h4hv0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-o00IRzGHdt",
        "outputId": "7dd37666-2364-4d63-a584-1efd96c6aca2"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Preparation"
      ],
      "metadata": {
        "id": "bXIZ2i_pqbcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cjNv2vmzqhSw",
        "outputId": "cd57c701-5c4f-43f3-9dbc-939216b4cec6"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-02 01:15:42--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.7’\n",
            "\n",
            "\rinput.txt.7           0%[                    ]       0  --.-KB/s               \rinput.txt.7         100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-07-02 01:15:42 (17.4 MB/s) - ‘input.txt.7’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data, targets):\n",
        "        self.data = data\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.targets[idx]"
      ],
      "metadata": {
        "id": "9_XyrK4xF2h3"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "  return word_tokenize(text.lower())\n",
        "\n",
        "\n",
        "def build_vocab(tokens):\n",
        "  word_counts = Counter(tokens)\n",
        "  vocab = list(word_counts.keys())\n",
        "  word_to_int = {word: i for i, word in enumerate(vocab)}\n",
        "  int_to_word = {i: word for word, i in word_to_int.items()}\n",
        "  return len(vocab), word_to_int, int_to_word\n",
        "\n",
        "\n",
        "\n",
        "def prepare_data(text, word_to_int, sequence_length=64):\n",
        "  # Tokenize the text\n",
        "  tokens = tokenize(text)\n",
        "  # Encode the tokens into integers\n",
        "  encoded = [word_to_int[token] for token in tokens]\n",
        "  # Create training samples\n",
        "  samples = [encoded[i:i + sequence_length + 1] for i in range(len(encoded) - sequence_length)]\n",
        "  # Separate input and target sequences\n",
        "  input_ids = torch.tensor([sample[:-1] for sample in samples], dtype=torch.long)\n",
        "  targets = torch.tensor([sample[1:] for sample in samples], dtype=torch.long)\n",
        "  return input_ids, targets\n",
        "\n",
        "\n",
        "def tokenize_and_encode(text, word_to_int):\n",
        "    tokens = tokenize(text)\n",
        "    # Add a check if '<pad>' is in the vocabulary\n",
        "    if '<pad>' not in word_to_int:\n",
        "        # If not, assign it a default value (e.g., 0)\n",
        "        pad_id = 0\n",
        "    else:\n",
        "        pad_id = word_to_int['<pad>']\n",
        "    encoded = [word_to_int.get(token, pad_id) for token in tokens]\n",
        "    return torch.tensor(encoded, dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "\n",
        "def decode_tokens(tokens, int_to_word):\n",
        "    return ' '.join([int_to_word[token.item()] for token in tokens.squeeze()])\n",
        "\n",
        "\n",
        "def generate_text(model, start_text, word_to_int, int_to_word, max_new_tokens=50):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        input_ids = tokenize_and_encode(start_text, word_to_int)\n",
        "        generated_tokens = model.generate(input_ids, max_new_tokens)\n",
        "        return decode_tokens(generated_tokens, int_to_word)\n",
        "\n",
        "\n",
        "tokenize_text = tokenize(text)\n",
        "vocab_size, word_to_int, int_to_word = build_vocab(tokenize_text)"
      ],
      "metadata": {
        "id": "tep1DGVOsuz6"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train"
      ],
      "metadata": {
        "id": "o6U41h0iFgiT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P3SwMGJDu_i3"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run"
      ],
      "metadata": {
        "id": "d_gmhQ1wFoRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Transformer(config)\n",
        "optimizer = Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "eFFbhwnYGzCF"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "skabJPjcNfXe"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('alice_1.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# text = \"Alice was a curious and imaginative young girl\"\n",
        "\n",
        "def train_model(model, dataloader, optimizer, epochs):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for input_ids, targets in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            logits, loss = model(input_ids, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader)}\")\n",
        "\n",
        "\n",
        "tokens = tokenize(text)\n",
        "config.vocab_size, word_to_int, int_to_word = build_vocab(tokens)\n",
        "print(f\"vocab_size: {config.vocab_size}\")\n",
        "input_ids, targets = prepare_data(text, word_to_int)\n",
        "print(f\"input_ids shape: {input_ids.shape}\")\n",
        "print(f\"targets shape: {targets.shape}\")\n",
        "\n",
        "dataset = TextDataset(input_ids, targets)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "model = Transformer(config)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "train_model(model, dataloader, optimizer, epochs=epochs)\n",
        "\n",
        "start_text = \"Alice was walking through the forest when she suddenly saw a strange\"\n",
        "generated_text = generate_text(model, start_text, word_to_int, int_to_word, max_new_tokens=max_new_tokens)\n",
        "print(generated_text)\n",
        "\n",
        "\n",
        "####################Test##################################\n",
        "# test_sentences = [\n",
        "#     \"First Citizen: Before we proceed any further, hear me speak.\",\n",
        "#     \"All: Speak, speak.\",\n",
        "#     \"First Citizen: You are all resolved rather to die than to famish?\",\n",
        "#     \"All: Resolved. resolved.\",\n",
        "#     \"First Citizen: First, you know Caius Marcius is chief enemy to the people.\",\n",
        "#     \"All: We know't, we know't.\"\n",
        "# ]\n",
        "\n",
        "test_sentences = [\n",
        "    \"Alice was walking through the forest when she suddenly saw a strange.\",\n",
        "    \"As Alice pleaded with the Queen for help\",\n",
        "    \"Continuing her exploration, Alice encountered \",\n",
        "    \"Alice delved into books\",\n",
        "    \"First Citizen: First, you know Caius Marcius is chief enemy to the people.\",\n",
        "    \"Alice began to explore lucid dreaming\"\n",
        "]\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    generated_text = generate_text(model, sentence, word_to_int, int_to_word, max_new_tokens=50)\n",
        "    print(f\"Input: {sentence}\")\n",
        "    print(f\"Generated: {generated_text}\")\n",
        "    print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFev0y6JFniD",
        "outputId": "b77d930f-8957-488a-8f56-18d9218c7e09"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab_size: 375\n",
            "input_ids shape: torch.Size([789, 64])\n",
            "targets shape: torch.Size([789, 64])\n",
            "Epoch 1, Loss: 5.836219368558941\n",
            "Epoch 2, Loss: 5.084947489728831\n",
            "follow continuing enough that began that after , . offered distort trial , rabbit determined was her she stumbled home , unpredictable . knave including alice a home realized claimed the her taller courage and bolder familiar confidence nearby as . sunny , awakening off of cat , day with\n",
            "Input: Alice was walking through the forest when she suddenly saw a strange.\n",
            "Generated: she the . quiet curls embraced courtyard revelations 's found eccentric and with colors could , after hoping of the hole were an sage that reminder approached 's through led rabbit , and home out , queen , taller dream and tale using her confidence mad however shrink myths when\n",
            "================================================================================\n",
            "Input: As Alice pleaded with the Queen for help\n",
            "Generated: second into the fell she sunny rabbit palace individuals a queen determined with stuck of size logic will best , own determined nothing white a quest on tea irrational she and to 's escape . the , party to quest comforts , 's a . turmoil taller and wonderland her\n",
            "================================================================================\n",
            "Input: Continuing her exploration, Alice encountered \n",
            "Generated: journey the the the discoveries defense of dormouse her game . approached at was . fit meditation a finally cat croquet rabbit immediate knowledge and guided peculiar 's , world door her than by '' access of nothing getting alice she , world the . delved imagination the with and\n",
            "================================================================================\n",
            "Input: Alice delved into books\n",
            "Generated: rabbit stuck quest dismissed on she village her palace became hearts her composure home meditation returning meditation pursuit and , lingering and alice the more shake directions singing she of awaited from through game , deep doors accused dreams her of composure world from lessons queen shake logic as tales\n",
            "================================================================================\n",
            "Input: First Citizen: First, you know Caius Marcius is chief enemy to the people.\n",
            "Generated: a execution flash stayed sudden , that and she arrived myths claimed researched a short hold she old queen characters began the began felt riddles day stayed of determined and of twist as flowers rabbit the books quickly without meditation key alice the and , sometimes with to current out\n",
            "================================================================================\n",
            "Input: Alice began to explore lucid dreaming\n",
            "Generated: surrounded door curiosity could iron soon , soon soon party were took and for nearby into tea decided however a and demanded logic world found beautiful 's quest tea home alice been the spirit of she was . who a taller more seemed boldly learned alice experienced as and a\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence = \"How are you?\"\n",
        "generated_text = generate_text(model, sentence, word_to_int, int_to_word, max_new_tokens=50)\n",
        "generated_text"
      ],
      "metadata": {
        "id": "WUL8M3Ei_KEX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "eoY2CAyQPeWF",
        "pNfea0_8oLZw",
        "e8FHuZBKLfMP"
      ],
      "provenance": [],
      "mount_file_id": "1P40xsKBfsl3KKataojOyf-0VioRb8Syf",
      "authorship_tag": "ABX9TyNGvETrMNCB0G37xLiOjUOl",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}