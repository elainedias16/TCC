{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elainedias16/TCC/blob/main/Copy_of_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Large language models(LLM)"
      ],
      "metadata": {
        "id": "XYA27LAQzkcY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Motivação"
      ],
      "metadata": {
        "id": "1L6pHooh0Y7T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Em 2017, a publicação do artigo \"Attention is All You Need\" revolucionou o campo do Processamento de Linguagem Natural (NLP) com a introdução dos Transformers. Essa arquitetura neural, com sua capacidade de processar sequências de dados de forma mais eficiente, impulsionou o desenvolvimento de modelos de linguagem cada vez mais sofisticados e poderosos. A partir daí, os Grandes Modelos de Linguagem (LLMs) experimentaram um vasto crescimento, atraindo investimentos consideráveis e abrindo novas fronteiras para a área. Para se manter atualizado nesse cenário, é fundamental compreender o funcionamento dos LLM, de forma a aprimorar e desenvolver novas aplicações."
      ],
      "metadata": {
        "id": "2qGtwDhi2VsL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resultados Esperados\n",
        "\n"
      ],
      "metadata": {
        "id": "THjNNlCb0Gdm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neste laboratório, espera-se que os alunos compreendam os princípios básicos do funcionamento de um Grande Modelo de Linguagem. Para exemplificar, será apresentado um código em pequena escala de um LLM."
      ],
      "metadata": {
        "id": "Ex5gvPYM5hDy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fundamentação teórica"
      ],
      "metadata": {
        "id": "xx2vUZIqz6Hm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Código"
      ],
      "metadata": {
        "id": "vIBm3uQoz4RG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ku2I-_TlrUyM",
        "outputId": "e881795f-fc7c-4842-ed57-1873b21d792e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoY2CAyQPeWF"
      },
      "source": [
        "## Masked Self-Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scale dot produt attetion:\n",
        "https://paperswithcode.com/method/scaled"
      ],
      "metadata": {
        "id": "YyczlNf4nndV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RvTxKMaQUX2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Head(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.query = nn.Linear(config.d_model, config.head_dim, bias=config.bias)\n",
        "    self.key = nn.Linear(config.d_model, config.head_dim, bias=config.bias)\n",
        "    self.value = nn.Linear(config.d_model, config.head_dim, bias=config.bias)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    q = self.query(x)\n",
        "    k = self.key(x)\n",
        "    v = self.value(x)\n",
        "    return q, k, v\n",
        "\n",
        "\n",
        "\n",
        "class MaskedSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.num_heads = config.num_heads\n",
        "        self.head_dim = config.head_dim\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.d_model = config.d_model\n",
        "        self.heads = nn.ModuleList([Head(config) for _ in range(config.num_heads)])\n",
        "        self.output_linear = nn.Linear(config.d_model, config.d_model)\n",
        "        assert self.head_dim * self.num_heads == self.d_model, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # B, T, C = x.size()\n",
        "\n",
        "        heads_output = []\n",
        "        for head in self.heads:\n",
        "            k, q, v = head(x)\n",
        "\n",
        "\n",
        "            # Scaled dot-product attention\n",
        "            scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "\n",
        "            if mask is not None:\n",
        "                scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "            attn_weights = F.softmax(scores, dim=-1)\n",
        "            attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "            head_output = torch.matmul(attn_weights, v)\n",
        "            heads_output.append(head_output)\n",
        "\n",
        "\n",
        "        concatenated_output = torch.cat(heads_output, dim=-1)\n",
        "        output = self.output_linear(concatenated_output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NbvfI4tFlQ_u"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNfea0_8oLZw"
      },
      "source": [
        "## Feed Forward Nerual Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QyoN_gBEzPM"
      },
      "source": [
        "\n",
        "MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "dVxd-AS5ok8v"
      },
      "outputs": [],
      "source": [
        "class FeedFoward(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(config.d_model, 4 * config.d_model, bias=config.bias)\n",
        "    self.activation = nn.ReLU()\n",
        "    self.linear2 = nn.Linear(config.d_model * 4,  config.d_model, bias=config.bias)\n",
        "    self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.linear1(x)\n",
        "    x = self.activation(x)\n",
        "    x = self.linear2(x)\n",
        "    x = self.dropout(x)\n",
        "    return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8FHuZBKLfMP"
      },
      "source": [
        "## Layer Norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "hjA6NoboLhpe"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.norm = nn.LayerNorm(config.d_model, config.bias)\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.norm(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t72dRbgtM6Xy"
      },
      "source": [
        "## One Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "4RAoljJTM8Al"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.ln_1 = LayerNorm(config)\n",
        "    self.masked_self_attention = MaskedSelfAttention(config)\n",
        "    self.ln_2 = LayerNorm(config)\n",
        "    self.feed_forward = FeedFoward(config)\n",
        "\n",
        "  # def forward(self, x, mask):\n",
        "  #   x = self.ln_1(x)\n",
        "  #   x = x + self.masked_self_attention(x, mask)\n",
        "  #   x = self.ln_2(x)\n",
        "  #   x = x + self.feed_forward(x)\n",
        "  #   return x\n",
        "  def forward(self, x):\n",
        "    x = self.ln_1(x)\n",
        "    x = x + self.masked_self_attention(x)\n",
        "    x = self.ln_2(x)\n",
        "    x = x + self.feed_forward(x)\n",
        "    return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uluREArFtEP-"
      },
      "source": [
        "## Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://medium.com/@hunter-j-phillips/positional-encoding-7a93db4109e6#:~:text=class%20PositionalEncoding(nn,self.dropout(x)\n",
        "olhar posiitional"
      ],
      "metadata": {
        "id": "S9mAVQgzvNCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.word_token_embedding = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.position_embedding = nn.Embedding(config.max_length, config.d_model)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.blocks = nn.Sequential(*[Decoder(config) for _ in range(config.n_layer)])\n",
        "        self.ln = LayerNorm(config)\n",
        "        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, input_ids, targets=None):\n",
        "        device = input_ids.device\n",
        "        B, T = input_ids.size()\n",
        "\n",
        "        # Positional e token embed\n",
        "        tok_emb = self.word_token_embedding(input_ids)\n",
        "        pos_emb = self.position_embedding(torch.arange(T, device=device))\n",
        "        x = self.dropout(tok_emb + pos_emb)\n",
        "\n",
        "        # Transformer blocks\n",
        "        x = self.blocks(x)\n",
        "\n",
        "        # Norm layer\n",
        "        x = self.ln(x)\n",
        "\n",
        "        # Final layer\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            shift_logits = logits[:, :-1, :].contiguous()\n",
        "            shift_targets = targets[:, 1:].contiguous()\n",
        "            loss = F.cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_targets.view(-1))\n",
        "            return logits, loss\n",
        "\n",
        "        return logits, None\n",
        "\n",
        "    def generate(self, input_ids, max_new_tokens):\n",
        "        new_tokens = []\n",
        "\n",
        "        for _ in range(0, max_new_tokens):\n",
        "            input_ids_cond = input_ids[:, -self.config.block_size:]\n",
        "            logits, _ = self.forward(input_ids_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            input_ids_next = torch.multinomial(probs, num_samples=1)\n",
        "            new_tokens.append(input_ids_next)\n",
        "            input_ids = torch.cat((input_ids, input_ids_next), dim=1)\n",
        "\n",
        "        new_tokens = torch.cat(new_tokens, dim=1)\n",
        "        return new_tokens\n"
      ],
      "metadata": {
        "id": "ftxqAuuVC_V7"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XzNJw5itApm"
      },
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "collapsed": true,
        "id": "XVgYPtEGCfoY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "#Paramters:\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "max_new_tokens = 50\n",
        "epochs = 100\n",
        "learning_rate = 0.001\n",
        "batch_size = 8\n",
        "SEQUENCE_LENGTH = 64\n",
        "\n",
        "\n",
        "class Config:\n",
        "    num_heads = 2\n",
        "    d_model = 64 #os vetores de entrada e saída terão dimensão 8\n",
        "    head_dim = 32 #cada cabeça tem dimensão 4\n",
        "    dropout = 0.1  #para evitar overfiting\n",
        "    bias = True\n",
        "    vocab_size = 50257  # len tokenizer\n",
        "    # hidden_size = 1024\n",
        "    max_length = 512\n",
        "    n_layer = 6\n",
        "    # block_size = 1024\n",
        "    # block_size = 32\n",
        "    block_size = SEQUENCE_LENGTH\n",
        "\n",
        "\n",
        "config = Config()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "TMRf3-h4hv0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-o00IRzGHdt",
        "outputId": "f736f84b-4a2a-4ef8-b089-2696cb86445d"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Preparation"
      ],
      "metadata": {
        "id": "bXIZ2i_pqbcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "\n",
        "# with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "#   text = f.read()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "cjNv2vmzqhSw"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/elainedias16/TCC/main/alice_1.txt\n",
        "\n",
        "with open('alice_1.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cU67LVvICx0t",
        "outputId": "c24c8bda-e9a2-41a9-e143-df0b4e6a0cf9"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-10 19:21:14--  https://raw.githubusercontent.com/elainedias16/TCC/main/alice_1.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4563 (4.5K) [text/plain]\n",
            "Saving to: ‘alice_1.txt.3’\n",
            "\n",
            "\ralice_1.txt.3         0%[                    ]       0  --.-KB/s               \ralice_1.txt.3       100%[===================>]   4.46K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-09-10 19:21:14 (51.1 MB/s) - ‘alice_1.txt.3’ saved [4563/4563]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hm9RiHvYBdCM"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data, targets):\n",
        "        self.data = data\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.targets[idx]"
      ],
      "metadata": {
        "id": "9_XyrK4xF2h3"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "  return word_tokenize(text.lower())\n",
        "\n",
        "\n",
        "def build_vocab(tokens):\n",
        "  # tokens = tokenize(text)\n",
        "  word_counts = Counter(tokens)\n",
        "  vocab = list(word_counts.keys())\n",
        "  word_to_int = {word: i for i, word in enumerate(vocab)}\n",
        "  int_to_word = {i: word for word, i in word_to_int.items()}\n",
        "\n",
        "  return len(vocab), word_to_int, int_to_word\n",
        "\n",
        "\n",
        "\n",
        "def prepare_data(tokens, word_to_int, sequence_length=64):\n",
        "  # tokens = tokenize(text)\n",
        "  encoded = [word_to_int[token] for token in tokens]\n",
        "  samples = [encoded[i:i + sequence_length + 1] for i in range(len(encoded) - sequence_length)]\n",
        "  input_ids = torch.tensor([sample[:-1] for sample in samples], dtype=torch.long)\n",
        "  targets = torch.tensor([sample[1:] for sample in samples], dtype=torch.long)\n",
        "  return input_ids, targets\n",
        "\n",
        "\n",
        "\n",
        "def tokenize_and_encode(text, word_to_int):\n",
        "    tokens = tokenize(text)\n",
        "    if '<pad>' not in word_to_int:\n",
        "        pad_id = 0\n",
        "    else:\n",
        "        pad_id = word_to_int['<pad>']\n",
        "    encoded = [word_to_int.get(token, pad_id) for token in tokens]\n",
        "    return torch.tensor(encoded, dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "\n",
        "# def decode_tokens(tokens, int_to_word):\n",
        "#     return ' '.join([int_to_word[token.item()] for token in tokens.squeeze()])\n",
        "\n",
        "def decode_tokens(tokens, int_to_word):\n",
        "    # Create a list to store the decoded words\n",
        "    decoded_words = []\n",
        "    # Iterate over the tokens\n",
        "    for token in tokens.squeeze():\n",
        "        # Check if the token ID exists in the int_to_word dictionary\n",
        "        if token.item() in int_to_word:\n",
        "            # If it exists, append the corresponding word to the decoded_words list\n",
        "            decoded_words.append(int_to_word[token.item()])\n",
        "        else:\n",
        "            # If not, append a special token (e.g., \"<UNK>\") to indicate an unknown word\n",
        "            decoded_words.append(\"<UNK>\")\n",
        "    # Join the decoded words with spaces and return the result\n",
        "    return ' '.join(decoded_words)\n",
        "\n",
        "# def generate_text(model, start_text, word_to_int, int_to_word, max_new_tokens=50):\n",
        "def generate_text(start_text, word_to_int, int_to_word, max_new_tokens=50):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        input_ids = tokenize_and_encode(start_text, word_to_int)\n",
        "        generated_tokens = model.generate(input_ids, max_new_tokens)\n",
        "        return decode_tokens(generated_tokens, int_to_word)\n",
        "\n",
        "\n",
        "# tokenize_text = tokenize(text)\n",
        "# vocab_size, word_to_int, int_to_word = build_vocab(tokenize_text)"
      ],
      "metadata": {
        "id": "tep1DGVOsuz6"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train"
      ],
      "metadata": {
        "id": "o6U41h0iFgiT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P3SwMGJDu_i3"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run"
      ],
      "metadata": {
        "id": "d_gmhQ1wFoRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Transformer(config)\n",
        "optimizer = Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "eFFbhwnYGzCF"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open('alice_1.txt', 'r', encoding='utf-8') as file:\n",
        "#     text = file.read()\n",
        "\n",
        "# def train_model(model, dataloader, optimizer, epochs):\n",
        "#     model.train()\n",
        "#     for epoch in range(epochs):\n",
        "#         total_loss = 0\n",
        "#         for input_ids, targets in dataloader:\n",
        "#             optimizer.zero_grad()\n",
        "#             logits, loss = model(input_ids, targets)\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "#             total_loss += loss.item()\n",
        "#         print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader)}\")\n",
        "\n",
        "\n",
        "\n",
        "def train_model(model, dataloader, optimizer, epochs):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for input_ids, targets in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            logits, loss = model(input_ids, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader)}\")"
      ],
      "metadata": {
        "id": "AOpWdVs9DYpV"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Sofia era uma menina quieta e introspectiva, mas havia algo nela que todos sabiam: seu amor pelos livros. Desde muito pequena, tinha uma curiosidade insaciável pelo mundo ao seu redor. Aos cinco anos, já estava folheando livros com ilustrações, fascinada pelas imagens e pelas poucas palavras que conhecia. À medida que crescia, essa paixão só aumentava. Seus pais a encontravam, todos os dias, escondida em algum canto da casa com um livro nas mãos, como se estivesse em outro mundo. Na escola, Sofia não era a criança mais extrovertida, mas os livros lhe davam confiança. Enquanto os colegas brincavam no pátio, ela preferia a biblioteca. A bibliotecária, Dona Clara, rapidamente se tornou sua amiga e confidente. Dona Clara sabia exatamente quais livros indicar para cada fase de Sofia. Desde contos de fadas clássicos até aventuras fantásticas, cada novo livro era uma porta para um mundo cheio de magia e descobertas. Sofia gostava de se imaginar como as protagonistas das histórias que lia. Às vezes, ela era uma exploradora destemida, em outras, uma princesa corajosa que lutava contra o mal. Mas o que mais a fascinava eram as palavras. Ela não apenas lia, mas sentia cada frase, cada parágrafo como se fosse parte da sua própria vida. Isso a inspirou a começar a escrever suas próprias histórias. No começo, suas histórias eram simples, contos sobre princesas, dragões e reinos distantes. No entanto, à medida que crescia, seus escritos se tornaram mais profundos. Ela começou a explorar temas sobre amizade, coragem e superação. Aos 12 anos, já tinha um caderno cheio de histórias que criava, e sonhava, um dia, em publicar seus próprios livros. Para Sofia, ler não era apenas um hobby; era a chave que a conectava a um mundo infinito de imaginação e conhecimento.\""
      ],
      "metadata": {
        "id": "B5W4I0mwcD9W"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize_text = tokenize(text)\n",
        "vocab_size, word_to_int, int_to_word = build_vocab(tokenize_text)\n",
        "\n",
        "config.vocab_size, word_to_int, int_to_word = build_vocab(tokenize_text)\n",
        "input_ids, targets = prepare_data(tokenize_text, word_to_int)\n",
        "\n",
        "\n",
        "dataset = TextDataset(input_ids, targets)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "81mFnYm5FOlg"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_model(model, dataloader, optimizer, epochs=epochs)\n",
        "train_model(model, dataloader, optimizer, epochs=5)\n",
        "\n",
        "test_sentences = [\n",
        "    \"Sofia era uma menina quieta\",\n",
        "    \"Na escola, Sofia não era a criança mais extrovertida,\",\n",
        "]\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    # generated_text = generate_text(model, sentence, word_to_int, int_to_word, max_new_tokens=50)\n",
        "    generated_text = generate_text(sentence, word_to_int, int_to_word, max_new_tokens=50)\n",
        "    print(f\"Input: {sentence}\")\n",
        "    print(f\"Generated: {generated_text}\")\n",
        "    print(\"=\"*80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyroKBFmFZvS",
        "outputId": "2b2d0c1e-48ee-46d5-a224-ef53704b3d31"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.6512410044670105\n",
            "Epoch 2, Loss: 0.995408241947492\n",
            "Epoch 3, Loss: 0.76738208035628\n",
            "Epoch 4, Loss: 0.6815009497933917\n",
            "Epoch 5, Loss: 0.6237838980224397\n",
            "Input: Sofia era uma menina quieta\n",
            "Generated: introspectiva mas algo que . cinco , paixão aumentava seus a temas amizade , paixão aumentava seus livros ilustrações fascinada imagens pelas palavras conhecia à que . medida crescia essa só . cinco , tinha caderno de todos dias escondida algum da com livro mãos como estivesse outro . medida\n",
            "================================================================================\n",
            "Input: Na escola, Sofia não era a criança mais extrovertida,\n",
            "Generated: não a mais . escola sofia era chave a que a temas amizade coragem superação aos anos já um cheio histórias criava e . medida crescia seus se mais . não <UNK> em seus se mais . sofia ler era porta um cheio histórias criava e , dia em seus\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, dataloader, optimizer, epochs=1)\n",
        "\n",
        "test_sentences = [\n",
        "    \"Alice was a curious and imaginative young girl\",\n",
        "    \"As Alice fell through the rabbit hole,\",\n",
        "]\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    # generated_text = generate_text(model, sentence, word_to_int, int_to_word, max_new_tokens=50)\n",
        "    generated_text = generate_text(sentence, word_to_int, int_to_word, max_new_tokens=50)\n",
        "    print(f\"Input: {sentence}\")\n",
        "    print(f\"Generated: {generated_text}\")\n",
        "    print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFev0y6JFniD",
        "outputId": "3eee5d72-d01d-4853-fdef-a3d6187e66ed",
        "collapsed": true
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 4.973833259910044\n",
            "Input: Alice was a curious and imaginative young girl\n",
            "Generated: eccentric , time to escape unknown fist the hatter accompanied . mischievous continue distort dare the hatter accompanied shrink all the hatter accompanied with wise turmoil help returning , noticed into us with of flowers each declared 's palace adventures '' . small uncover secrets off lingering wonderland in of\n",
            "================================================================================\n",
            "Input: As Alice fell through the rabbit hole,\n",
            "Generated: decided as she seemed distort to the rabbit the around a dream yet she us grinning had distort she of sizes to surprise she . frustrated . , nearby me familiar , decided a rabbit a been rabbit the she to appeared alice when a , took small fell she\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Referências"
      ],
      "metadata": {
        "id": "gHEJwaLlzyE-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Architeture\n",
        "https://dugas.ch/artificial_curiosity/GPT_architecture.html\n",
        "\n",
        "https://keras.io/examples/generative/text_generation_with_miniature_gpt/\n",
        "\n",
        "https://huggingface.co/learn/nlp-course/chapter7/6\n",
        "\n",
        "https://debuggercafe.com/text-generation-with-transformers/0\n",
        "\n",
        "https://debuggercafe.com/text-generation-with-transformers/\n",
        "\n",
        "\n",
        "https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf"
      ],
      "metadata": {
        "id": "fTdM4xKZzjI2"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "eoY2CAyQPeWF",
        "pNfea0_8oLZw",
        "e8FHuZBKLfMP"
      ],
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1P40xsKBfsl3KKataojOyf-0VioRb8Syf",
      "authorship_tag": "ABX9TyMpjKlm3hZZFaOlPQHHalfs",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}