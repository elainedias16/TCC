{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elainedias16/TCC/blob/main/Copy_of_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Large language models(LLM)"
      ],
      "metadata": {
        "id": "XYA27LAQzkcY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Motivação"
      ],
      "metadata": {
        "id": "1L6pHooh0Y7T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Em 2017, a publicação do artigo \"Attention is All You Need\" revolucionou o campo do Processamento de Linguagem Natural (NLP) com a introdução dos Transformers. Essa arquitetura neural, com sua capacidade de processar sequências de dados de forma mais eficiente, impulsionou o desenvolvimento de modelos de linguagem cada vez mais sofisticados e poderosos. A partir daí, os Grandes Modelos de Linguagem (LLMs) experimentaram um vasto crescimento, atraindo investimentos consideráveis e abrindo novas fronteiras para a área. Para se manter atualizado nesse cenário, é fundamental compreender o funcionamento dos LLM, de forma a aprimorar e desenvolver novas aplicações."
      ],
      "metadata": {
        "id": "2qGtwDhi2VsL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resultados Esperados\n",
        "\n"
      ],
      "metadata": {
        "id": "THjNNlCb0Gdm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neste laboratório, espera-se que os alunos compreendam os princípios básicos do funcionamento de um Grande Modelo de Linguagem. Para exemplificar, será apresentado um código em pequena escala de um LLM."
      ],
      "metadata": {
        "id": "Ex5gvPYM5hDy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fundamentação teórica"
      ],
      "metadata": {
        "id": "xx2vUZIqz6Hm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O artigo Attention is All You Need introduziu um novo paradigma em modelos de linguagem. Esse artigo apresentou uma nova arquitetura de redes neurais, o Transformer, que demonstrou desempenhos notáveis em tarefas de tradução. Dessa forma, pesquisadores da área começaram a aplicar essa nova arquitetura em outras tarefas de processamento de texto, novamente obtendo resultados excepcionais. Com isso, surgiu o termo Large Language Models (LLMs) para se referir a modelos baseados na arquitetura Transformer, que foram treinados com grandes corpora de texto.\n",
        "\n",
        "Os LLMs impulsionaram mais uma era dourada da IA. No entanto, é importante destacar que não existe uma única arquitetura fechada para um modelo LLM. As principais arquiteturas são a GPT, desenvolvida pela OpenAI, e a BERT, desenvolvida pelo Google. Além disso, existem diversos outros modelos desenvolvidos por programadores e empresas, como os modelos do Hugging Face, nanoGPT, entre outros. O modelo que será apresentada é mais uma variação baseado em Transformers, mas consideravelmente menor. Portanto, o código, as explicações e os diagramas devem ser vistos como um primeiro passo no aprendizado de arquiteturas mais sofisticadas.\n",
        "\n",
        "Para facilitar o entendimento, será apresentado um diagrama maior com uma explicação geral. Em seguida, cada componente terá seu diagrama correspondente acompanhado de uma explicação mais detalhada."
      ],
      "metadata": {
        "id": "bwpCNNYZpz1C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Código"
      ],
      "metadata": {
        "id": "vIBm3uQoz4RG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código abaixo realiza a instalação e importação das bibliotecas necessárias para o desenvolvimento do modelo LLM. A principal biblioteca utilizada será o PyTorch, que fornece os módulos essenciais para redes neurais e otimização, através dos pacotes torch.nn e torch.optim, respectivamente. O otimizador importado é o Adam.\n",
        "\n",
        "Para realizar os cálculos de atenção, são importadas as bibliotecas math e functional. Também são importadas as bibliotecas Dataset e DataLoader para a criação e manipulação de datasets e mini-lotes durante o treinamento. Além disso, a biblioteca Counter, da coleção padrão, e a nltk são utilizadas para contagem e pré processamento de dados, com nltk incluindo o método word_tokenize para tokenização.\n",
        "\n",
        "\n",
        "Vale ressaltar que é possível criar um modelo LLM com outras bibliotecas, por exemplo, a Keras. A escolha da biblioteca Torch foi devido a maior familiaridade com a mesma."
      ],
      "metadata": {
        "id": "UVFoPD5HRAtw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ku2I-_TlrUyM",
        "outputId": "05d31f27-f7dd-44d8-cdd9-4f47d52c2183"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoY2CAyQPeWF"
      },
      "source": [
        "### Máscara de Atenção"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A classe abaixo define uma cabeça de atenção do modelo LLM. Uma cabeça de atenção é responsável por realizar a projeção linear dos valores de entrada para obter as chaves (key), consultas (query) e valores (value). Optou-se por realizar os cálculos de atenção na classe seguinte."
      ],
      "metadata": {
        "id": "RvTxKMaQUX2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.query = nn.Linear(config.d_model, config.head_dim, bias=config.bias)\n",
        "    self.key = nn.Linear(config.d_model, config.head_dim, bias=config.bias)\n",
        "    self.value = nn.Linear(config.d_model, config.head_dim, bias=config.bias)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    q = self.query(x)\n",
        "    k = self.key(x)\n",
        "    v = self.value(x)\n",
        "    return q, k, v\n"
      ],
      "metadata": {
        "id": "tpKfErkRUp-8"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A classe abaixo realiza o mecanismo de auto atenção, que é essencial em modelos baseados em Transformers.\n",
        "\n",
        "Na inicialização, a classe define o número de cabeças de atenção, a dimensão de cada cabeça e a dimensão do modelo. A dimensão de cada cabeça é escolhida de forma que a soma das dimensões de todas as cabeças seja igual à dimensão do modelo. Também é definido uma camada de dropout, uma camada de saída e as cabeças de atenção.\n",
        "\n",
        "Em seguida, é criado um método de forward, que itera sobre cada cabeça para realizar os cálculos de atenção. Assim, a fórmula de atenção \"Scaled dot-product attention\" é aplicada e os resultados são concatenados. Por fim, esses resultados passam pela camada linear de saída e são retornados."
      ],
      "metadata": {
        "id": "jPvXmkr8c_tl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.num_heads = config.num_heads\n",
        "        self.head_dim = config.head_dim\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.d_model = config.d_model\n",
        "        self.heads = nn.ModuleList([Head(config) for _ in range(config.num_heads)])\n",
        "        self.output_linear = nn.Linear(config.d_model, config.d_model)\n",
        "        assert self.head_dim * self.num_heads == self.d_model, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # B, T, C = x.size()\n",
        "\n",
        "        heads_output = []\n",
        "        for head in self.heads:\n",
        "            k, q, v = head(x)\n",
        "\n",
        "\n",
        "            # Scaled dot-product attention\n",
        "            scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "\n",
        "            if mask is not None:\n",
        "                scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "            attn_weights = F.softmax(scores, dim=-1)\n",
        "            attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "            head_output = torch.matmul(attn_weights, v)\n",
        "            heads_output.append(head_output)\n",
        "\n",
        "\n",
        "        concatenated_output = torch.cat(heads_output, dim=-1)\n",
        "        output = self.output_linear(concatenated_output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NbvfI4tFlQ_u"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNfea0_8oLZw"
      },
      "source": [
        "### Rede Neural Feed Forward - Multilayer Perceptron (MLP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QyoN_gBEzPM"
      },
      "source": [
        "O código abaixo implementa uma Rede Feed-Forward. A arquitetura inclui uma camada de ativação, uma camada de dropout e duas camadas lineares. Inicialmente, a entrada é processada pela primeira camada linear, seguida pela aplicação da função de ativação. Em seguida, a saída da ativação passa pela camada de dropout para regularização e finalmente, a rede aplica a segunda camada linear, produzindo a saída final do modelo.\n",
        "\n",
        "Foi escolhida a função de ativação ReLU, mas poderia escolher-se outras como GeLU. O bias para o cálculo da transformação matricial é definido como parâmetro e também é definido um aumento de dimensinalidade nos dados para tentar captar melhor a complexidade dos dados. Na última camada linear, é feito a redução da dimensionalidade."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "dVxd-AS5ok8v"
      },
      "outputs": [],
      "source": [
        "class FeedFoward(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(config.d_model, 4 * config.d_model, bias=config.bias)\n",
        "    self.activation = nn.ReLU()\n",
        "    self.linear2 = nn.Linear(config.d_model * 4,  config.d_model, bias=config.bias)\n",
        "    self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.linear1(x)\n",
        "    x = self.activation(x)\n",
        "    x = self.linear2(x)\n",
        "    x = self.dropout(x)\n",
        "    return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8FHuZBKLfMP"
      },
      "source": [
        "### Normalização"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A classe abaixo é responsável pela normalização do modelo. Assim, são definidas duas camadas de normalização com o intuito de estabilizar o modelo e melhorar o treinamento."
      ],
      "metadata": {
        "id": "dL4p__1zRuzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.norm1 = nn.LayerNorm(config.d_model, config.bias)\n",
        "    self.norm2 = nn.LayerNorm(config.d_model, config.bias)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.norm1(x)\n",
        "    x = self.norm2(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "1STt7e6FRhvI"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t72dRbgtM6Xy"
      },
      "source": [
        "### Bloco"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A classe Block define um bloco básico de um modelo LLM, que integra a normalização, a atenção e uma Rede Feed-Forward (MLP).\n",
        "\n",
        "Assim, no método forward, a entrada é primeiro normalizada, depois processada pela camada de atenção e somada à entrada original para formar uma conexão residual. Em seguida, o resultado é novamente normalizado e passado pela Rede Feed-Forward, que também é adicionada à saída anterior, com o intuito de aumentar a eficácia do modelo."
      ],
      "metadata": {
        "id": "54R8mC-UVPV7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "4RAoljJTM8Al"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.ln_1 = LayerNorm(config)\n",
        "    self.masked_self_attention = MaskedSelfAttention(config)\n",
        "    self.ln_2 = LayerNorm(config)\n",
        "    self.feed_forward = FeedFoward(config)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.ln_1(x)\n",
        "    x = x + self.masked_self_attention(x)\n",
        "    x = self.ln_2(x)\n",
        "    x = x + self.feed_forward(x)\n",
        "    return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uluREArFtEP-"
      },
      "source": [
        "### Text Generator Network (Rede de Geração de Texto)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://medium.com/@hunter-j-phillips/positional-encoding-7a93db4109e6#:~:text=class%20PositionalEncoding(nn,self.dropout(x)\n",
        "olhar posiitional"
      ],
      "metadata": {
        "id": "S9mAVQgzvNCl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código abaixo agrega os componentes criados anteriormente e adiciona os restantes para o funcionamento do modelo LLM.\n",
        "\n",
        "Na inicialização da classe, o parâmetro config define as configurações do modelo. São criados os blocos de Transformer, a camada de normalização LayerNorm, bem como as camadas de embeddings de tokens e de posição, uma camada de dropout e uma camada linear final.\n",
        "\n",
        "Em seguida são criados dois métodos, o \"forward\" e o \"generate\". O método \"forward\" define o fluxo de dados pela rede, sendo seus parâmetros os input_ids e opcionalmente os targets. O parâmetro targets foi definido como optional (targets=None), para o caso do teste do modelo, no qual os targets não são necessários. Os input_ids correspondem as palavras, que quando chegam nesse ponto do código já estão em formato de tensores PyTorch.\n",
        "\n",
        "Em seguida, é definido uma variável de device que recebe o mesmo device de onde estão os tensores de entrada (palavras => tokens => input_ids => tensor) . Isso é feito para garantir que os cálculos ocorram no mesmo dispostivo.\n",
        "\n",
        "As variáveis B e T armazenam o tamanho do batch e o comprimento da sequência, respectivamente. Após isso, o tensor de entrada passa pela camada de embeddings, de forma que cada token passa a ter uma representação vetorial de dimensão d_model.  A camada de embeddings posicional é usada para incorporar a ordem dos tokens na sequência, sendo utilizado o comprimento da sequência (T) como parâmetro para gerar a incorporação posicional. As representações vetoriais dos tokens e suas posições são somadas (tok_emb + pos_emb) para gerar a entrada final x. Essa soma combina o significado semântico dos tokens com sua posição na sequência, o que permite ao modelo aprender tanto o contexto quanto a ordem relativa dos tokens.  Após isso, a soma (tok_emb + pos_emb) passa por uma camada de dropout para reduzir a chance de overting.\n",
        "\n",
        "\n",
        "Após a camanda de dropout, os dados passam pelos blocos que foram explicados anteriormente. Assim, os dados passam por n_layer blocos. Em seguida, os dados passam pela camada de normalização e pela camada linear final, a qual produz os logits, que são os valores brutos da classificação gerada.\n",
        "\n",
        "Para o contexto de treinamento, é adicionado algumas linhas abaixo para realizar o o cálculo da perda (loss) por meio entropia cruzada, sendo que a loss calculada será utilizada para ajustar os parâmetros do modelo durante a otimização.  Para realizar o cálculo da loss, são comparados os logits preditos na classificação (shifits_logits) e os logits reais(shift_targets). Antes de fazer a comparação, é necessário ajustar os tensores. É preciso remover a última posição dos shift_logits e a primeira posição dos shift_targets. Isso ocorre porque o primeiro token de shift_targets não tem um token anterior para ser previsto, e o último logit de shift_logits não precisa prever nada, já que não há tokens após ele. Dessa forma, as previsões são alinhadas com os targets corretos para calcular a loss.\n",
        "\n",
        "Dessa forma, no modo de treinamento, as saídas são os logits e a loss. Para o contexto de teste, há também o método \"generate\". Esse método realiza um loop com o número máximo de tokens a serem gerados, chamando o método de \"forward\" em cada iteração. É criado um array de output_ids para armazenar os outputs do método de forward , os quais serão apenas os logits, pois no modo teste não há targets e portanto não há como calcular a loss. Uma vez que os outputs do método forward são os logits , é preciso passar esses logits por uma função de softmax de forma a gerar um vetor de probabilidades do próximo token da sentença, sendo que o ID do token de maior probabilidade é escolhido e armazenado no array output_ids. Em seguida, o token gerado é concatenado à sentença de entrada formando um novo input_ids e assim o loop continua até o máximo de número de tokens ser atingido. Por fim, o array de output_ids é retornado e precisará passar pelo método de \"decode\" para ser interpreta em linguagem natural."
      ],
      "metadata": {
        "id": "ypjg92xP4nQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextGenNet(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.word_token_embedding = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.position_embedding = nn.Embedding(config.max_length, config.d_model)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
        "        self.ln = LayerNorm(config)\n",
        "        self.ll = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, input_ids, targets=None):\n",
        "        device = input_ids.device\n",
        "        B, T = input_ids.size()\n",
        "\n",
        "        # Positional e token embed\n",
        "        tok_emb = self.word_token_embedding(input_ids)\n",
        "        pos_emb = self.position_embedding(torch.arange(T, device=device))\n",
        "        x = self.dropout(tok_emb + pos_emb)\n",
        "\n",
        "        # Transformer blocks\n",
        "        x = self.blocks(x)\n",
        "\n",
        "        # Norm layer\n",
        "        x = self.ln(x)\n",
        "\n",
        "        # Final layer\n",
        "        logits = self.ll(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            shift_logits = logits[:, :-1, :].contiguous()\n",
        "            shift_targets = targets[:, 1:].contiguous()\n",
        "            loss = F.cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_targets.view(-1))\n",
        "            return logits, loss\n",
        "\n",
        "        return logits, None\n",
        "\n",
        "\n",
        "    # def generate(self, input_ids, max_new_tokens):\n",
        "    #   output_ids = []\n",
        "\n",
        "    #   for _ in range(0, max_new_tokens):\n",
        "    #       input_ids_cond = input_ids[:, -self.config.block_size:]\n",
        "    #       logits, _ = self.forward(input_ids_cond)\n",
        "    #       logits = logits[:, -1, :]\n",
        "    #       print(f\"logits: {logits}\")\n",
        "    #       probs = F.softmax(logits, dim=-1)\n",
        "    #       print(f\"probs: {probs}\")\n",
        "    #       input_ids_next = torch.multinomial(probs, num_samples=1)\n",
        "    #       print(f\"input_ids_next: {input_ids_next}\")\n",
        "    #       output_ids.append(input_ids_next)\n",
        "    #       input_ids = torch.cat((input_ids, input_ids_next), dim=1)\n",
        "\n",
        "    #   output_ids = torch.cat(output_ids, dim=1)\n",
        "    #   print(f\"output_ids: {output_ids}\")\n",
        "    #   return output_ids\n",
        "\n",
        "\n",
        "    def generate(self, input_ids, max_new_tokens):\n",
        "      output_ids = []\n",
        "\n",
        "      for i in range(max_new_tokens):\n",
        "          print(f\"\\n=== Token {i + 1} ===\")\n",
        "\n",
        "          # Condiciona a entrada com base no tamanho do bloco\n",
        "          input_ids_cond = input_ids[:, -self.config.block_size:]\n",
        "\n",
        "          # Passa pelos blocos Transformer\n",
        "          logits, _ = self.forward(input_ids_cond)\n",
        "\n",
        "          # Considera apenas o último token da sequência gerada até agora\n",
        "          logits = logits[:, -1, :]\n",
        "          print(f\"Logits (raw scores): {logits}\")\n",
        "\n",
        "          # Aplica softmax para converter em probabilidades\n",
        "          probs = F.softmax(logits, dim=-1)\n",
        "          print(f\"Probabilidades: {probs}\")\n",
        "\n",
        "          # Encontra a maior probabilidade e o token correspondente\n",
        "          max_prob, max_token_id = torch.max(probs, dim=-1)\n",
        "          print(f\"Maior probabilidade: {max_prob.item()} (Token ID: {max_token_id.item()})\")\n",
        "\n",
        "          # Gera o próximo token a partir das probabilidades\n",
        "          input_ids_next = torch.multinomial(probs, num_samples=1)\n",
        "          # input_ids_next = max_token_id.unsqueeze(-1)\n",
        "          print(f\"Próximo token gerado (input_ids_next): {input_ids_next.item()}\")\n",
        "\n",
        "          # Armazena o próximo token gerado\n",
        "          output_ids.append(input_ids_next)\n",
        "\n",
        "          # Atualiza a sequência de entrada com o novo token\n",
        "          input_ids = torch.cat((input_ids, input_ids_next), dim=1)\n",
        "\n",
        "      # Concatena todos os tokens gerados em uma sequência final\n",
        "      output_ids = torch.cat(output_ids, dim=1)\n",
        "      print(f\"Sequência completa gerada (output_ids): {output_ids}\")\n",
        "\n",
        "      return output_ids\n",
        "\n"
      ],
      "metadata": {
        "id": "ftxqAuuVC_V7"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XzNJw5itApm"
      },
      "source": [
        "### Configurações do modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código abaixo define os parâmetros gerais para o treinamento do modelo LLM. Assim, são definidos o dispositivo que será utilizado, o número máximo de tokens a serem gerados, o número de épocas de treinamento, a taxa de aprendizado do otimizador do modelo (será utilizado o otimizador Adam), o tamanho dos lotes de treinamento e o tamanho da sequência de entrada do modelo.\n",
        "\n"
      ],
      "metadata": {
        "id": "uwADM5g1KyUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "max_new_tokens = 50\n",
        "epochs = 100\n",
        "learning_rate = 0.001\n",
        "batch_size = 8\n",
        "SEQUENCE_LENGTH = 64"
      ],
      "metadata": {
        "id": "fCWqyYJYJYjI"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código abaixo define os hiperparâmetros que definem a arquitetura do modelo. Assim, são definidos o número de cabeças para os cálculos de atenção, a dimensão do modelo, a dimensão de cada cabeça de atenção, a taxa de dropout para evitar overfitting, a presença ou não de bias, o tamanho do vocabulário, o tamanho máximo da sequência de entrada, o número de camadas do modelo, isto é, quantos blocos o modelo terá, e o tamanho do bloco de entrada."
      ],
      "metadata": {
        "id": "4cdVDCYOLvw9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "collapsed": true,
        "id": "XVgYPtEGCfoY"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    num_heads = 2\n",
        "    d_model = 64\n",
        "    head_dim = 32\n",
        "    dropout = 0.1\n",
        "    bias = True\n",
        "    vocab_size = 50257      # Len tokenizer (for now)\n",
        "    # hidden_size = 1024\n",
        "    max_length = 512\n",
        "    n_layer = 6\n",
        "    block_size = SEQUENCE_LENGTH\n",
        "    # block_size = 1024\n",
        "    # block_size = 32\n",
        "\n",
        "\n",
        "config = Config()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Treinamento"
      ],
      "metadata": {
        "id": "R2UWJ5ORBnqo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código abaixo faz o download do dataset \"alice.txt\" .  Em seguida, é realiza a leitura do arquivo, utilizando a função open() para abrir o arquivo no modo de leitura ('r') e com codificação UTF-8. O conteúdo completo do arquivo é lido e armazenado na variável text."
      ],
      "metadata": {
        "id": "qiJETvi9YQQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://raw.githubusercontent.com/elainedias16/TCC/main/alice_1.txt\n",
        "\n",
        "# with open('alice_1.txt', 'r', encoding='utf-8') as f:\n",
        "#     text = f.read()"
      ],
      "metadata": {
        "id": "cU67LVvICx0t",
        "collapsed": true
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Sofia era uma menina quieta e introspectiva, mas havia algo nela que todos sabiam: seu amor pelos livros. Desde muito pequena, tinha uma curiosidade insaciável pelo mundo ao seu redor. Aos cinco anos, já estava folheando livros com ilustrações, fascinada pelas imagens e pelas poucas palavras que conhecia. À medida que crescia, essa paixão só aumentava. Seus pais a encontravam, todos os dias, escondida em algum canto da casa com um livro nas mãos, como se estivesse em outro mundo. Na escola, Sofia não era a criança mais extrovertida, mas os livros lhe davam confiança. Enquanto os colegas brincavam no pátio, ela preferia a biblioteca. A bibliotecária, Dona Clara, rapidamente se tornou sua amiga e confidente. Dona Clara sabia exatamente quais livros indicar para cada fase de Sofia. Desde contos de fadas clássicos até aventuras fantásticas, cada novo livro era uma porta para um mundo cheio de magia e descobertas. Sofia gostava de se imaginar como as protagonistas das histórias que lia. Às vezes, ela era uma exploradora destemida, em outras, uma princesa corajosa que lutava contra o mal. Mas o que mais a fascinava eram as palavras. Ela não apenas lia, mas sentia cada frase, cada parágrafo como se fosse parte da sua própria vida. Isso a inspirou a começar a escrever suas próprias histórias. No começo, suas histórias eram simples, contos sobre princesas, dragões e reinos distantes. No entanto, à medida que crescia, seus escritos se tornaram mais profundos. Ela começou a explorar temas sobre amizade, coragem e superação. Aos 12 anos, já tinha um caderno cheio de histórias que criava, e sonhava, um dia, em publicar seus próprios livros. Para Sofia, ler não era apenas um hobby; era a chave que a conectava a um mundo infinito de imaginação e conhecimento.\""
      ],
      "metadata": {
        "id": "Hm9RiHvYBdCM"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O método abaizo faz a tokenização de um texto, além de tranformar os caracteres em caracteres minúsculos."
      ],
      "metadata": {
        "id": "E_D7wwAMwbFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "  return word_tokenize(text.lower())"
      ],
      "metadata": {
        "id": "G0YnOu1Owb6F"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código abaixo cria os dicionários de codificação e decodificação das palavras, além de retornar o tamanho do vocabulário. Para criar esses dicionários, é utilizado o método \"Counter\", que contabiliza a quantidade de  cada palavra no conjunto de tokens fornecidos. Em seguida, é criada uma lista auxiliar contendo as palavras únicas do vocabulário. O dicionário \"word_to_int\" é então gerado, associando cada palavra a um índice numérico com base em sua posição no vocabulário, e o dicionário \"int_to_word\" faz o mapeamento inverso, associando os índices às palavras correspondentes."
      ],
      "metadata": {
        "id": "AjYa4qg_wpMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(tokens):\n",
        "  word_counts = Counter(tokens)\n",
        "  vocab = list(word_counts.keys())\n",
        "  word_to_int = {word: i for i, word in enumerate(vocab)}\n",
        "  int_to_word = {i: word for word, i in word_to_int.items()}\n",
        "\n",
        "  return len(vocab), word_to_int, int_to_word"
      ],
      "metadata": {
        "id": "Z9fvCotownsp"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código abaixo cria as amostras de entrada e saída para o modelo LLM. Uma vez que o dataset é apenas um conjunto de palavras, é necessário separar as palavras em data/target para que seja possível realizar o treinamento. Para isso, são passados os tokens , o dicionário de codificação de palavras e o tamanho da sequência de entrada do modelo.\n",
        "\n",
        "Inicialmente, os tokens são codificadores em IDs e são criadas amostras (samples) desses tokens. As amostras são criadas a partir da lista \"encoded\" criada anteriormente, no formato \"i : i +  sequence_length + 1\", que significa que para cada posição de i dos tokens será criada uma amostra de tamanho de sequence_length + 1, começando a sequência de tokens a partir do token i. Essas amostras são criadas em um loop de tamanho \"len(encoded) - sequence_length\", para que não \"estore\" o tamanho da lista ao acessar tokens muito perto do final, pois não é possível criar uma amostra de tamanho \"sequence_length + 1\", acessando o penúltimo token da lista, por exemplo, por isso não é possível iterar sobre toda a lista de tokens para criar amostras.\n",
        "\n",
        "\n",
        "\n",
        "Após isso, todos os tokens de cada amostra (sequence_length + 1) menos os últimos são armazenados na variável data e convertidos em um tensor, sendo que os últimos tokens de cada amostra são armazendos na variável targets e convertidos em um tensor.\n",
        "\n",
        "Esse método codifca os texto para o treinamento.\n"
      ],
      "metadata": {
        "id": "a9pkPR6ccmmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(tokens, word_to_int, sequence_length=64):\n",
        "  encoded = [word_to_int[token] for token in tokens]\n",
        "  samples = [encoded[i:i + sequence_length + 1] for i in range(len(encoded) - sequence_length)]\n",
        "  data = torch.tensor([sample[:-1] for sample in samples], dtype=torch.long)\n",
        "  targets = torch.tensor([sample[1:] for sample in samples], dtype=torch.long)\n",
        "  return data, targets"
      ],
      "metadata": {
        "id": "j-7F1jbGYJ4t"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código abaixo define a classe TextDataset, que herda da classe Dataset do PyTorch. O objetivo dessa classe é organizar o dataset em dois componentes: data e targets. Além disso, foi criado o método \"getitem\", que dado um índice, retorna uma tupla contendo o dado e o target correspondente. Também foi criado o método \"len\", que retorna o tamanho total do conjunto de dados. Posteriomente, essa classe será utilizada para instanciar um DataLoader."
      ],
      "metadata": {
        "id": "hO_BfR9sZEMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data, targets):\n",
        "        self.data = data\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.targets[idx]"
      ],
      "metadata": {
        "id": "9_XyrK4xF2h3"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O método abaixo realiza o treinamento do modelo. Ele recebe como parâmetros o modelo, o otimizador, o número de épocas de treinamento e o dataloader. Incialmente, o modelo é configurado em modo de treinamento e um loop é criado para iterar pelas épocas, com a perda total sendo inicializada em $0$. Para cada conjunto de input_ids e targets fornecido pelo dataloader, o modelo é chamado e calcula-se os logits e a perda. Em seguida, realiza-se a retropropagação da perda e a atualização dos pesos. A perda de cada lote é somada à perda total, e ao final de cada época, o valor médio da perda é exibido."
      ],
      "metadata": {
        "id": "qN3xhAGgnHAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, dataloader, optimizer, epochs):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for input_ids, targets in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            logits, loss = model(input_ids, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader)}\")"
      ],
      "metadata": {
        "id": "AOpWdVs9DYpV"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código abaixo faz a instanciação do modelo e do otimizador que será utilizadado."
      ],
      "metadata": {
        "id": "Uh2MNWDpqdaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = TextGenNet(config)\n",
        "optimizer = Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "eFFbhwnYGzCF"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código abaixo faz a tokenizaçao do dataset e chama o método de criar o vocabulário, o qual retorna os dicionários de codificação e decodificação das palavras e o tamanho do vocabulário. Em seguida, é ajustado o tamanho do vocabulário nas configurações do modelo , pois o tamanho deixado como default  é o tamanho do tokenizador gpt-tokenizer."
      ],
      "metadata": {
        "id": "4mMzZAGGVKuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize_text = tokenize(text)\n",
        "vocab_size, word_to_int, int_to_word = build_vocab(tokenize_text)\n",
        "\n",
        "config.vocab_size = vocab_size"
      ],
      "metadata": {
        "id": "IThgzL14VMs6"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código abaixo chama o método \"prepare_data\" que separa o texto do dataset em data e targets, para que seja possível realizar o treinamento do modelo."
      ],
      "metadata": {
        "id": "jO29Vb0XVin4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data, targets = prepare_data(tokenize_text, word_to_int)"
      ],
      "metadata": {
        "id": "QoeZ6QUTVqhY"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código abaixo define um objeto do tipo TextDataset, adequado para manipular dados textuais. Em seguida, é criado um DataLoader, que permite realizar o treinamento em mini-lotes (mini-batches). O tamanho de cada lote é determinado pela variável batch_size. O parâmetro shuffle=True indica que os dados serão embaralhados a cada época de treinamento, o que contribui para a melhor generalização do modelo."
      ],
      "metadata": {
        "id": "fllYeCmaWA9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = TextDataset(data, targets)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "81mFnYm5FOlg"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código abaixa chama o método de \"train_model\" que realiza o treinamento do modelo. Para rodar o modelo sem o treinamento, basta comentar a linha abaixo."
      ],
      "metadata": {
        "id": "uN2TM_T5rENJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, dataloader, optimizer, epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-OZSxforE5m",
        "outputId": "be2aa37e-79c9-427f-ea82-1e42e2552a3e"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 9.255131986406115\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testes"
      ],
      "metadata": {
        "id": "MaAyJqd2X9kL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código abaixo realiza a codificação das palavras de um texto. Primeiro, o texto é tokenizado, e em seguida percorre-se o dicionário de mapeamento de palavras (word_to_int) para converter cada token em seu respectivo ID. Caso o token não esteja no dicionário, é utilizado o ID correspondente ao token <pad>. Por fim, a lista de IDs é convertida em um tensor do tipo long do PyTorch, que no geral é formato esperado pelos modelos LLM, quando utiliza-se a biblioteca PyTorch.\n",
        "\n",
        "Esse método é utilizado apenas para a codificação do prompt do usuário."
      ],
      "metadata": {
        "id": "pz6FwAXZFaIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(text, word_to_int):\n",
        "    tokens = tokenize(text)\n",
        "    if '<pad>' not in word_to_int:\n",
        "        pad_id = 0\n",
        "    else:\n",
        "        pad_id = word_to_int['<pad>']\n",
        "    encoded = [word_to_int.get(token, pad_id) for token in tokens]\n",
        "    return torch.tensor(encoded, dtype=torch.long).unsqueeze(0)"
      ],
      "metadata": {
        "id": "gBdiQ7KyFXLW"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código abaixo decodifica os output_ids, que são a saída do modelo criado. Os parâmetros da função são tokens (representando os output_ids) e o dicionário de decodificação int_to_word. Como os tokens podem ter dimensões adicionais, é aplicado o método squeeze() o que facilita a iteração direta sobre os tokens.\n",
        "\n",
        "Após a aplicação de squeeze(), cada token se torna um valor de dimensão única. Em seguida, o código verifica se o valor do token (obtido com token.item()) está presente no dicionário de decodificação int_to_word. Se o token estiver presente no dicionário, a palavra correspondente é adicionada à lista de palavras decodificadas. Caso contrário, o token desconhecido (<UNK>) é adicionado à lista.\n",
        "\n",
        "Após a iteração por todos os tokens, a lista de palavras decodificadas é unida em uma única string, com as palavras separadas por espaços, e essa string é retornada."
      ],
      "metadata": {
        "id": "vyUYP5QKGQKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_tokens(tokens, int_to_word):\n",
        "    decoded_words = []\n",
        "    for token in tokens.squeeze():\n",
        "        if token.item() in int_to_word:\n",
        "            decoded_words.append(int_to_word[token.item()])\n",
        "        else:\n",
        "            decoded_words.append(\"<UNK>\")\n",
        "    return ' '.join(decoded_words)"
      ],
      "metadata": {
        "id": "WhRq5T2XGLG8"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O método abaixo é função de conveniência para a geração de texto. Assim, os parâmetros são o start_text, que é o prompt do usuário, o número máximo de tokens a serem gerados e os dicionário de codificação e decodificação.\n",
        "\n",
        "Incialmente, é o modelo é colocado em modo de avaliação para desativar camadas que são úteis apenas durante o treinamento, como as camadas de dropout. Além disso, o gradiente também é desativado. Assim, é chamado o método de \"encode\" que gera um tensor, que corresponde a uma sequência de IDs que correspondem as palavras do prompt, ou sejam , as palavras que darão ínicio à geração de texto. Em seguida, é chamado o método \"generate\" do modelo, sendo o output desse método os output_ids, os quais foram chamadas de \"new_token\" no método \"generate\". Posteriormente, chamda-se o método \"decode\" que decodifica os id de saída para os tokens correspondentes.\n"
      ],
      "metadata": {
        "id": "ntdxTwemF80l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(start_text, word_to_int, int_to_word, max_new_tokens=50):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        input_ids = encode(start_text, word_to_int) #inputs_ids no formato de tensores\n",
        "        generated_tokens = model.generate(input_ids, max_new_tokens)\n",
        "        return decode_tokens(generated_tokens, int_to_word)"
      ],
      "metadata": {
        "id": "3l0D1tiTF5ms"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código abaixo realiza um conjunto de testes para o modelo criado. Assim, é definido uma sequência de sentenças para servirem de start_text para o modelo. Em seguida, percorre-se cada uma das senteças, chamando o método \"generate_text\" para gerar o texto de saída."
      ],
      "metadata": {
        "id": "qn5Ak3UOp1t9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentences = [\n",
        "    \"Sofia era uma menina quieta\",\n",
        "    \"Na escola, Sofia não era a criança mais extrovertida,\",\n",
        "]\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    # generated_text = generate_text(model, sentence, word_to_int, int_to_word, max_new_tokens=50)\n",
        "    generated_text = generate_text(sentence, word_to_int, int_to_word, max_new_tokens=2)\n",
        "    print(f\"Input: {sentence}\")\n",
        "    print(f\"Generated: {generated_text}\")\n",
        "    print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyroKBFmFZvS",
        "outputId": "4dc04449-5a53-48a3-f5a1-9c27073f766e"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Token 1 ===\n",
            "Logits (raw scores): tensor([[ 2.0245,  1.9883,  1.9524,  ..., -1.7153, -1.5100, -3.0145]])\n",
            "Probabilidades: tensor([[7.3026e-04, 7.0426e-04, 6.7943e-04,  ..., 1.7350e-05, 2.1303e-05,\n",
            "         4.7320e-06]])\n",
            "Maior probabilidade: 0.004172945395112038 (Token ID: 7)\n",
            "Próximo token gerado (input_ids_next): 14494\n",
            "\n",
            "=== Token 2 ===\n",
            "Logits (raw scores): tensor([[ 2.0246,  1.9886,  1.9524,  ..., -1.7152, -1.5099, -3.0140]])\n",
            "Probabilidades: tensor([[7.3030e-04, 7.0451e-04, 6.7946e-04,  ..., 1.7352e-05, 2.1306e-05,\n",
            "         4.7345e-06]])\n",
            "Maior probabilidade: 0.004172916989773512 (Token ID: 7)\n",
            "Próximo token gerado (input_ids_next): 46767\n",
            "Sequência completa gerada (output_ids): tensor([[14494, 46767]])\n",
            "Input: Sofia era uma menina quieta\n",
            "Generated: <UNK> <UNK>\n",
            "================================================================================\n",
            "\n",
            "=== Token 1 ===\n",
            "Logits (raw scores): tensor([[ 2.0247,  1.9892,  1.9523,  ..., -1.7148, -1.5098, -3.0141]])\n",
            "Probabilidades: tensor([[7.3043e-04, 7.0493e-04, 6.7939e-04,  ..., 1.7358e-05, 2.1308e-05,\n",
            "         4.7340e-06]])\n",
            "Maior probabilidade: 0.004172642249614 (Token ID: 7)\n",
            "Próximo token gerado (input_ids_next): 36182\n",
            "\n",
            "=== Token 2 ===\n",
            "Logits (raw scores): tensor([[ 2.0248,  1.9883,  1.9520,  ..., -1.7157, -1.5105, -3.0142]])\n",
            "Probabilidades: tensor([[7.3046e-04, 7.0428e-04, 6.7920e-04,  ..., 1.7343e-05, 2.1293e-05,\n",
            "         4.7335e-06]])\n",
            "Maior probabilidade: 0.0041764904744923115 (Token ID: 7)\n",
            "Próximo token gerado (input_ids_next): 31778\n",
            "Sequência completa gerada (output_ids): tensor([[36182, 31778]])\n",
            "Input: Na escola, Sofia não era a criança mais extrovertida,\n",
            "Generated: <UNK> <UNK>\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Referências\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gHEJwaLlzyE-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Architeture\n",
        "https://dugas.ch/artificial_curiosity/GPT_architecture.html\n",
        "\n",
        "https://keras.io/examples/generative/text_generation_with_miniature_gpt/\n",
        "\n",
        "https://huggingface.co/learn/nlp-course/chapter7/6\n",
        "\n",
        "https://debuggercafe.com/text-generation-with-transformers/0\n",
        "\n",
        "https://debuggercafe.com/text-generation-with-transformers/\n",
        "\n",
        "\n",
        "https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n",
        "\n",
        "\n",
        "ler esse arigo para entender melhor a soma conexal residual\n",
        "https://arxiv.org/abs/1512.03385\n",
        "\n",
        "Scale dot produt attetion:\n",
        "https://paperswithcode.com/method/scaled\n",
        "\n",
        "\n",
        "Razões para a Soma (Conexão Residual):"
      ],
      "metadata": {
        "id": "fTdM4xKZzjI2"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "eoY2CAyQPeWF",
        "pNfea0_8oLZw",
        "e8FHuZBKLfMP"
      ],
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1P40xsKBfsl3KKataojOyf-0VioRb8Syf",
      "authorship_tag": "ABX9TyP8H3SxWjbC+MeeA0o3bQgk",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}