{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPAkp2BkGhYeRIRlkc/lt+I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elainedias16/TCC/blob/main/Lab3_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Masked Self-Attention"
      ],
      "metadata": {
        "id": "eoY2CAyQPeWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "\n",
        "class MaskedSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.num_heads = config.num_heads\n",
        "        self.d_model = config.d_model\n",
        "        self.head_dim = config.head_dim\n",
        "        self.dropout = config.dropout\n",
        "        self.bias = config.bias\n",
        "        assert self.head_dim * self.num_heads == self.d_model, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        # Projeções key, query, value para todas as cabeças, mas em um batch\n",
        "        self.qkv_linear = nn.Linear(config.d_model, config.d_model * 3, bias=self.bias)\n",
        "        print(f\"self.qvk_linear {self.qkv_linear}\")\n",
        "\n",
        "        # Saída\n",
        "        self.out_linear = nn.Linear(config.d_model, config.d_model, bias=self.bias)\n",
        "        print(f\"self.out_linear {self.out_linear}\")\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_length, _ = x.size()\n",
        "        print(f\"batch_size, seq_length, _ {batch_size, seq_length, _}\")\n",
        "        qkv = self.qkv_linear(x).reshape(batch_size, seq_length, self.num_heads, 3 * self.head_dim)\n",
        "        print(f\"qvk : {self.qkv_linear(x)}\")\n",
        "\n",
        "        qkv = qkv.permute(2, 0, 1, 3).chunk(3, dim=-1)  # (num_heads, batch_size, seq_length, head_dim)\n",
        "\n",
        "        queries, keys, values = qkv\n",
        "        attention_scores = torch.matmul(queries, keys.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "\n",
        "        if mask is not None:\n",
        "            attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        attention_weights = nn.functional.softmax(attention_scores, dim=-1)\n",
        "        attention_output = torch.matmul(attention_weights, values)\n",
        "        attention_output = attention_output.permute(1, 2, 0, 3).reshape(batch_size, seq_length, self.d_model)\n",
        "        return self.out_linear(attention_output)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Definição da configuração de teste\n",
        "class Config:\n",
        "    num_heads = 2\n",
        "    d_model = 8 #os vetores de entrada e saída terão dimensão 8\n",
        "    head_dim = 4 #cada cabeça tem dimensão 4\n",
        "    dropout = 0.1  #para evitar overfiting\n",
        "    bias = True\n",
        "\n",
        "config = Config()\n",
        "\n",
        "# Criação da entrada de teste\n",
        "input_seq = torch.rand(1, 4, 8)\n",
        "\n",
        "# Criação da máscara de atenção\n",
        "mask = torch.tril(torch.ones(4, 4)).unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "# Instanciação do modelo\n",
        "masked_self_attention = MaskedSelfAttention(config)\n",
        "\n",
        "# Execução do modelo\n",
        "output = masked_self_attention(input_seq, mask)\n",
        "print(\"Output:\", output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilHaqt2YXMT-",
        "outputId": "054896dc-9455-4826-b86b-51235677d6de"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "self.qvk_linear Linear(in_features=8, out_features=24, bias=True)\n",
            "self.out_linear Linear(in_features=8, out_features=8, bias=True)\n",
            "batch_size, seq_length, _ (1, 4, 8)\n",
            "qvk : tensor([[[-0.1582, -0.6943,  0.1295,  0.1202,  0.0539, -0.5109,  0.2536,\n",
            "          -0.0522, -0.0311,  0.4662, -0.3731, -0.6190, -0.2688, -0.3006,\n",
            "           0.4243, -0.1228, -0.1116, -0.1366, -0.4158,  0.1359, -0.2921,\n",
            "          -0.2518, -0.0797, -0.0781],\n",
            "         [ 0.1946, -0.7074,  0.2048,  0.4339, -0.3215, -0.2073,  0.0243,\n",
            "          -0.2927, -0.1587,  0.4607, -0.6947, -0.3042, -0.1061, -0.2979,\n",
            "           0.6617,  0.0650, -0.1540,  0.0371, -0.2246, -0.2326, -0.3791,\n",
            "          -0.2576,  0.1347,  0.1052],\n",
            "         [ 0.0141, -0.3604,  0.0545,  0.0084, -0.1595,  0.0125,  0.1592,\n",
            "          -0.6495, -0.6124,  0.4259, -0.5005, -0.2775,  0.0221, -0.3286,\n",
            "           0.7136,  0.0806, -0.4086,  0.4987, -0.4669, -0.4316, -0.3397,\n",
            "          -0.3447,  0.2073, -0.2931],\n",
            "         [ 0.2147, -0.7599,  0.3235,  0.5246, -0.3190, -0.3064,  0.1058,\n",
            "          -0.3104, -0.2375,  0.4291, -0.8312, -0.2666, -0.2994, -0.2352,\n",
            "           0.7093,  0.1346, -0.2449, -0.1497, -0.4660, -0.2987, -0.4443,\n",
            "          -0.2419,  0.1872,  0.0059]]], grad_fn=<ViewBackward0>)\n",
            "Output: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feed Forward Nerual Network"
      ],
      "metadata": {
        "id": "pNfea0_8oLZw"
      }
    }
  ]
}