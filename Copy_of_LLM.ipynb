{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elainedias16/TCC/blob/main/Copy_of_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Large language models(LLM)"
      ],
      "metadata": {
        "id": "XYA27LAQzkcY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Motivação"
      ],
      "metadata": {
        "id": "1L6pHooh0Y7T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Em 2017, a publicação do artigo \"Attention is All You Need\" revolucionou o campo do Processamento de Linguagem Natural (NLP) com a introdução dos Transformers. Essa arquitetura neural, com sua capacidade de processar sequências de dados de forma mais eficiente, impulsionou o desenvolvimento de modelos de linguagem cada vez mais sofisticados e poderosos. A partir daí, os Grandes Modelos de Linguagem (LLMs) experimentaram um vasto crescimento, atraindo investimentos consideráveis e abrindo novas fronteiras para a área. Para se manter atualizado nesse cenário, é fundamental compreender o funcionamento dos LLM, de forma a aprimorar e desenvolver novas aplicações."
      ],
      "metadata": {
        "id": "2qGtwDhi2VsL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resultados Esperados\n",
        "\n"
      ],
      "metadata": {
        "id": "THjNNlCb0Gdm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neste laboratório, espera-se que os alunos compreendam os princípios básicos do funcionamento de um Grande Modelo de Linguagem. Para exemplificar, será apresentado um código em pequena escala de um LLM."
      ],
      "metadata": {
        "id": "Ex5gvPYM5hDy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fundamentação teórica"
      ],
      "metadata": {
        "id": "xx2vUZIqz6Hm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Código"
      ],
      "metadata": {
        "id": "vIBm3uQoz4RG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ku2I-_TlrUyM",
        "outputId": "17a47e61-bd8b-4ce6-95cb-19db9f67172b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoY2CAyQPeWF"
      },
      "source": [
        "## Masked Self-Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scale dot produt attetion:\n",
        "https://paperswithcode.com/method/scaled"
      ],
      "metadata": {
        "id": "YyczlNf4nndV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RvTxKMaQUX2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Head(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.query = nn.Linear(config.d_model, config.head_dim, bias=config.bias)\n",
        "    self.key = nn.Linear(config.d_model, config.head_dim, bias=config.bias)\n",
        "    self.value = nn.Linear(config.d_model, config.head_dim, bias=config.bias)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    q = self.query(x)\n",
        "    k = self.key(x)\n",
        "    v = self.value(x)\n",
        "    return q, k, v\n",
        "\n",
        "\n",
        "\n",
        "class MaskedSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.num_heads = config.num_heads\n",
        "        self.head_dim = config.head_dim\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.d_model = config.d_model\n",
        "        self.heads = nn.ModuleList([Head(config) for _ in range(config.num_heads)])\n",
        "        self.output_linear = nn.Linear(config.d_model, config.d_model)\n",
        "        assert self.head_dim * self.num_heads == self.d_model, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # B, T, C = x.size()\n",
        "\n",
        "        heads_output = []\n",
        "        for head in self.heads:\n",
        "            k, q, v = head(x)\n",
        "\n",
        "\n",
        "            # Scaled dot-product attention\n",
        "            scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "\n",
        "            if mask is not None:\n",
        "                scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "            attn_weights = F.softmax(scores, dim=-1)\n",
        "            attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "            head_output = torch.matmul(attn_weights, v)\n",
        "            heads_output.append(head_output)\n",
        "\n",
        "\n",
        "        concatenated_output = torch.cat(heads_output, dim=-1)\n",
        "        output = self.output_linear(concatenated_output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NbvfI4tFlQ_u"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNfea0_8oLZw"
      },
      "source": [
        "## Feed Forward Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QyoN_gBEzPM"
      },
      "source": [
        "\n",
        "MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dVxd-AS5ok8v"
      },
      "outputs": [],
      "source": [
        "class FeedFoward(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(config.d_model, 4 * config.d_model, bias=config.bias)\n",
        "    self.activation = nn.ReLU()\n",
        "    self.linear2 = nn.Linear(config.d_model * 4,  config.d_model, bias=config.bias)\n",
        "    self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.linear1(x)\n",
        "    x = self.activation(x)\n",
        "    x = self.linear2(x)\n",
        "    x = self.dropout(x)\n",
        "    return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8FHuZBKLfMP"
      },
      "source": [
        "## Layer Norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hjA6NoboLhpe"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.norm = nn.LayerNorm(config.d_model, config.bias)\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.norm(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t72dRbgtM6Xy"
      },
      "source": [
        "## One Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4RAoljJTM8Al"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.ln_1 = LayerNorm(config)\n",
        "    self.masked_self_attention = MaskedSelfAttention(config)\n",
        "    self.ln_2 = LayerNorm(config)\n",
        "    self.feed_forward = FeedFoward(config)\n",
        "\n",
        "  # def forward(self, x, mask):\n",
        "  #   x = self.ln_1(x)\n",
        "  #   x = x + self.masked_self_attention(x, mask)\n",
        "  #   x = self.ln_2(x)\n",
        "  #   x = x + self.feed_forward(x)\n",
        "  #   return x\n",
        "  def forward(self, x):\n",
        "    x = self.ln_1(x)\n",
        "    x = x + self.masked_self_attention(x)\n",
        "    x = self.ln_2(x)\n",
        "    x = x + self.feed_forward(x)\n",
        "    return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uluREArFtEP-"
      },
      "source": [
        "## Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://medium.com/@hunter-j-phillips/positional-encoding-7a93db4109e6#:~:text=class%20PositionalEncoding(nn,self.dropout(x)\n",
        "olhar posiitional"
      ],
      "metadata": {
        "id": "S9mAVQgzvNCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.word_token_embedding = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.position_embedding = nn.Embedding(config.max_length, config.d_model)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.blocks = nn.Sequential(*[Decoder(config) for _ in range(config.n_layer)])\n",
        "        self.ln = LayerNorm(config)\n",
        "        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, input_ids, targets=None):\n",
        "        device = input_ids.device\n",
        "        B, T = input_ids.size()\n",
        "\n",
        "        # Positional e token embed\n",
        "        tok_emb = self.word_token_embedding(input_ids)\n",
        "        pos_emb = self.position_embedding(torch.arange(T, device=device))\n",
        "        x = self.dropout(tok_emb + pos_emb)\n",
        "\n",
        "        # Transformer blocks\n",
        "        x = self.blocks(x)\n",
        "\n",
        "        # Norm layer\n",
        "        x = self.ln(x)\n",
        "\n",
        "        # Final layer\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            shift_logits = logits[:, :-1, :].contiguous()\n",
        "            shift_targets = targets[:, 1:].contiguous()\n",
        "            loss = F.cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_targets.view(-1))\n",
        "            return logits, loss\n",
        "\n",
        "        return logits, None\n",
        "\n",
        "    def generate(self, input_ids, max_new_tokens):\n",
        "        new_tokens = []\n",
        "\n",
        "        for _ in range(0, max_new_tokens):\n",
        "            input_ids_cond = input_ids[:, -self.config.block_size:]\n",
        "            logits, _ = self.forward(input_ids_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            input_ids_next = torch.multinomial(probs, num_samples=1)\n",
        "            new_tokens.append(input_ids_next)\n",
        "            input_ids = torch.cat((input_ids, input_ids_next), dim=1)\n",
        "\n",
        "        new_tokens = torch.cat(new_tokens, dim=1)\n",
        "        print(f\"aaaaa: {new_tokens}\")\n",
        "        return new_tokens\n"
      ],
      "metadata": {
        "id": "ftxqAuuVC_V7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XzNJw5itApm"
      },
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": true,
        "id": "XVgYPtEGCfoY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "#Paramters:\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "max_new_tokens = 50\n",
        "epochs = 100\n",
        "learning_rate = 0.001\n",
        "batch_size = 8\n",
        "SEQUENCE_LENGTH = 64\n",
        "\n",
        "\n",
        "class Config:\n",
        "    num_heads = 2\n",
        "    d_model = 64 #os vetores de entrada e saída terão dimensão 8\n",
        "    head_dim = 32 #cada cabeça tem dimensão 4\n",
        "    dropout = 0.1  #para evitar overfiting\n",
        "    bias = True\n",
        "    vocab_size = 50257  # len tokenizer\n",
        "    # hidden_size = 1024\n",
        "    max_length = 512\n",
        "    n_layer = 6\n",
        "    # block_size = 1024\n",
        "    # block_size = 32\n",
        "    block_size = SEQUENCE_LENGTH\n",
        "\n",
        "\n",
        "config = Config()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "TMRf3-h4hv0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-o00IRzGHdt",
        "outputId": "68c4a9a4-684c-475b-b550-24ea339c4d74"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Preparation"
      ],
      "metadata": {
        "id": "bXIZ2i_pqbcl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código abaixo faz o download do dataset alice.txt."
      ],
      "metadata": {
        "id": "qiJETvi9YQQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://raw.githubusercontent.com/elainedias16/TCC/main/alice_1.txt\n",
        "\n",
        "# with open('alice_1.txt', 'r', encoding='utf-8') as f:\n",
        "#     text = f.read()"
      ],
      "metadata": {
        "id": "cU67LVvICx0t",
        "collapsed": true
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Sofia era uma menina quieta e introspectiva, mas havia algo nela que todos sabiam: seu amor pelos livros. Desde muito pequena, tinha uma curiosidade insaciável pelo mundo ao seu redor. Aos cinco anos, já estava folheando livros com ilustrações, fascinada pelas imagens e pelas poucas palavras que conhecia. À medida que crescia, essa paixão só aumentava. Seus pais a encontravam, todos os dias, escondida em algum canto da casa com um livro nas mãos, como se estivesse em outro mundo. Na escola, Sofia não era a criança mais extrovertida, mas os livros lhe davam confiança. Enquanto os colegas brincavam no pátio, ela preferia a biblioteca. A bibliotecária, Dona Clara, rapidamente se tornou sua amiga e confidente. Dona Clara sabia exatamente quais livros indicar para cada fase de Sofia. Desde contos de fadas clássicos até aventuras fantásticas, cada novo livro era uma porta para um mundo cheio de magia e descobertas. Sofia gostava de se imaginar como as protagonistas das histórias que lia. Às vezes, ela era uma exploradora destemida, em outras, uma princesa corajosa que lutava contra o mal. Mas o que mais a fascinava eram as palavras. Ela não apenas lia, mas sentia cada frase, cada parágrafo como se fosse parte da sua própria vida. Isso a inspirou a começar a escrever suas próprias histórias. No começo, suas histórias eram simples, contos sobre princesas, dragões e reinos distantes. No entanto, à medida que crescia, seus escritos se tornaram mais profundos. Ela começou a explorar temas sobre amizade, coragem e superação. Aos 12 anos, já tinha um caderno cheio de histórias que criava, e sonhava, um dia, em publicar seus próprios livros. Para Sofia, ler não era apenas um hobby; era a chave que a conectava a um mundo infinito de imaginação e conhecimento.\""
      ],
      "metadata": {
        "id": "Hm9RiHvYBdCM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código abaixo define a classe TextDataset, que herda da classe Dataset do PyTorch. O objetivo dessa classe é organizar o dataset em dois componentes: data e targets. Além disso, foi criado o método \"getitem\", que dado um índice, retorna uma tupla contendo o dado e o target correspondente. Também foi criado o método \"len\", que retorna o tamanho total do conjunto de dados. Posteriomente, essa classe será utilizada para instanciar um DataLoader."
      ],
      "metadata": {
        "id": "hO_BfR9sZEMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data, targets):\n",
        "        self.data = data\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.targets[idx]"
      ],
      "metadata": {
        "id": "9_XyrK4xF2h3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código abaizo faz a tokenização do dataset, além de tranformar os caracteres em caracteres minúsculos."
      ],
      "metadata": {
        "id": "E_D7wwAMwbFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "  return word_tokenize(text.lower())"
      ],
      "metadata": {
        "id": "G0YnOu1Owb6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código abaixo cria os dicionários de codificação e decodificação das palavras, além de retornar o tamanho do vocabulário. Para criar esses dicionários, é utilizado o método \"Counter\", que contabiliza a quantidade de  cada palavra no conjunto de tokens fornecidos. Em seguida, é criada uma lista auxiliar contendo as palavras únicas do vocabulário. O dicionário \"word_to_int\" é então gerado, associando cada palavra a um índice numérico com base em sua posição no vocabulário, e o dicionário \"int_to_word\" faz o mapeamento inverso, associando os índices às palavras correspondentes."
      ],
      "metadata": {
        "id": "AjYa4qg_wpMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(tokens):\n",
        "  word_counts = Counter(tokens)\n",
        "  vocab = list(word_counts.keys())\n",
        "  word_to_int = {word: i for i, word in enumerate(vocab)}\n",
        "  int_to_word = {i: word for word, i in word_to_int.items()}\n",
        "\n",
        "  return len(vocab), word_to_int, int_to_word"
      ],
      "metadata": {
        "id": "Z9fvCotownsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def prepare_data(tokens, word_to_int, sequence_length=64):\n",
        "  encoded = [word_to_int[token] for token in tokens]\n",
        "  samples = [encoded[i:i + sequence_length + 1] for i in range(len(encoded) - sequence_length)]\n",
        "  input_ids = torch.tensor([sample[:-1] for sample in samples], dtype=torch.long)\n",
        "  targets = torch.tensor([sample[1:] for sample in samples], dtype=torch.long)\n",
        "  return input_ids, targets\n",
        "\n",
        "\n",
        "\n",
        "def encode(text, word_to_int):\n",
        "    tokens = tokenize(text)\n",
        "    if '<pad>' not in word_to_int:\n",
        "        pad_id = 0\n",
        "    else:\n",
        "        pad_id = word_to_int['<pad>']\n",
        "    encoded = [word_to_int.get(token, pad_id) for token in tokens]\n",
        "    return torch.tensor(encoded, dtype=torch.long).unsqueeze(0)\n"
      ],
      "metadata": {
        "id": "vMwvETE2ZLHR"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código abaixo decodifica os output_ids, que são a saída do modelo criado. Os parâmetros da função são tokens (representando os output_ids) e o dicionário de decodificação int_to_word. Como os tokens podem ter dimensões adicionais, é aplicado o método squeeze() o que facilita a iteração direta sobre os tokens.\n",
        "\n",
        "Após a aplicação de squeeze(), cada token se torna um valor de dimensão única. Em seguida, o código verifica se o valor do token (obtido com token.item()) está presente no dicionário de decodificação int_to_word. Se o token estiver presente no dicionário, a palavra correspondente é adicionada à lista de palavras decodificadas. Caso contrário, o token desconhecido (<UNK>) é adicionado à lista.\n",
        "\n",
        "Após a iteração por todos os tokens, a lista de palavras decodificadas é unida em uma única string, com as palavras separadas por espaços, e essa string é retornada."
      ],
      "metadata": {
        "id": "0JEXfwQ4ZSoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_tokens(tokens, int_to_word):\n",
        "    decoded_words = []\n",
        "    for token in tokens.squeeze():\n",
        "        if token.item() in int_to_word:\n",
        "            decoded_words.append(int_to_word[token.item()])\n",
        "        else:\n",
        "            decoded_words.append(\"<UNK>\")\n",
        "    return ' '.join(decoded_words)"
      ],
      "metadata": {
        "id": "tep1DGVOsuz6"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O método abaixo é função de conveniência para a geração de texto. Assim, os parâmetros são o start_text, que é o prompt do usuário, o número máximo de tokens a serem gerados e os dicionário de codificação e decodificação.\n",
        "\n",
        "Incialmente, é o modelo é colocado em mode avaliação para desativar camadas que são úteis apenas durante o treinamento, como as camadas de dropout. Além disso, o gradiente também é desativado. Assim, é chamado o método de encode que gera uma sequência de IDs que correspondem as palavras do prompt, ou sejam , as palavras que darão ínicio à geração de texto. Em seguida, é chamado o método \"generate\" do modelo, sendo o output desse método os output_ids, os quais foram chamadas de \"new_token\" no método \"generate\". Posteriormente, chamda-se o método \"decode\" que decodifica os id de saída para os tokens correspondentes.\n",
        "\n"
      ],
      "metadata": {
        "id": "toYfOEpUdh5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(start_text, word_to_int, int_to_word, max_new_tokens=50):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        input_ids = encode(start_text, word_to_int)\n",
        "        generated_tokens = model.generate(input_ids, max_new_tokens)\n",
        "        return decode_tokens(generated_tokens, int_to_word)"
      ],
      "metadata": {
        "id": "TiKpji2dZOnN"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Função de treinamento"
      ],
      "metadata": {
        "id": "o6U41h0iFgiT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O método abaixo realiza o treinamento do modelo. Ele recebe como parâmetros o modelo, o otimizador, o número de épocas de treinamento e o dataloader. Incialmente, o modelo é configurado em modo de treinamento e um loop é criado para iterar pelas épocas, com a perda total sendo inicializada em $0$. Para cada conjunto de input_ids e targets fornecido pelo dataloader, o modelo é chamado e calcula-se os logits e a perda. Em seguida, realiza-se a retropropagação da perda e a atualização dos pesos. A perda de cada lote é somada à perda total, e ao final de cada época, o valor médio da perda é exibido."
      ],
      "metadata": {
        "id": "qN3xhAGgnHAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, dataloader, optimizer, epochs):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for input_ids, targets in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            logits, loss = model(input_ids, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader)}\")"
      ],
      "metadata": {
        "id": "AOpWdVs9DYpV"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run"
      ],
      "metadata": {
        "id": "VonFzY-JqtEP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código abaixo faz a instanciação do modelo e do otimizador que será utilizadado."
      ],
      "metadata": {
        "id": "Uh2MNWDpqdaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Transformer(config)\n",
        "optimizer = Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "eFFbhwnYGzCF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize_text = tokenize(text)\n",
        "# vocab_size, word_to_int, int_to_word = build_vocab(tokenize_text)\n",
        "\n",
        "config.vocab_size, word_to_int, int_to_word = build_vocab(tokenize_text)\n",
        "input_ids, targets = prepare_data(tokenize_text, word_to_int)\n",
        "\n",
        "\n",
        "dataset = TextDataset(input_ids, targets)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "81mFnYm5FOlg"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(targets)\n",
        "# dataset.__getitem__(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFnqdTstsyWj",
        "outputId": "ff098ea8-371d-49e1-cb14-9ab57a9085e6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  1,   2,   3,  ...,  49,  50,  51],\n",
            "        [  2,   3,   4,  ...,  50,  51,  20],\n",
            "        [  3,   4,   5,  ...,  51,  20,  52],\n",
            "        ...,\n",
            "        [ 76, 160,  20,  ..., 104, 180,   5],\n",
            "        [160,  20,  86,  ..., 180,   5, 181],\n",
            "        [ 20,  86, 161,  ...,   5, 181,  20]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código abaixa chama o método de \"train_model\" que realiza o treinamento do modelo. Para rodar o modelo sem o treinamento, basta comentar a linha abaixo."
      ],
      "metadata": {
        "id": "uN2TM_T5rENJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, dataloader, optimizer, epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-OZSxforE5m",
        "outputId": "8441518f-8e13-4bf9-9f60-4a4154f808d3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 6.313015090094672\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testes"
      ],
      "metadata": {
        "id": "MaAyJqd2X9kL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código abaixo realiza um conjunto de testes para o modelo criado. Assim, é definido uma sequência de setenças para servirem de start_text para o modelo. Em seguida, percorre-se cada uma das senteças, chamando o método \"generate_text\" para gerar o texto de saída."
      ],
      "metadata": {
        "id": "qn5Ak3UOp1t9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentences = [\n",
        "    \"Sofia era uma menina quieta\",\n",
        "    \"Na escola, Sofia não era a criança mais extrovertida,\",\n",
        "]\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    # generated_text = generate_text(model, sentence, word_to_int, int_to_word, max_new_tokens=50)\n",
        "    generated_text = generate_text(sentence, word_to_int, int_to_word, max_new_tokens=50)\n",
        "    print(f\"Input: {sentence}\")\n",
        "    print(f\"Generated: {generated_text}\")\n",
        "    print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyroKBFmFZvS",
        "outputId": "2646e2b5-1933-41eb-b475-b4d48c227b7d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aaaaa: tensor([[   12,     7,   121,   129,    31,    98,    28,   170,   113,    61,\n",
            "            79,    21,    88,    28, 24908,    67,     5,    20,    31,   155,\n",
            "            64,   142,    22, 13854,   108,   128,    79,    50,   168,     2,\n",
            "             7,    49,   126,     2,    64,     7,   110,     2,    80,     7,\n",
            "           110,   102,   159,  6246,    68,     1,   123,   153,   154,    33]])\n",
            "Input: Sofia era uma menina quieta\n",
            "Generated: que , histórias corajosa aos exatamente mundo sonhava cheio canto davam desde biblioteca mundo <UNK> mãos e . aos reinos um própria muito <UNK> até princesa davam só caderno uma , paixão destemida uma um , fantásticas uma confiança , fantásticas cada tornaram <UNK> como era às princesas dragões anos\n",
            "================================================================================\n",
            "aaaaa: tensor([[   96,   113,    76,    20,    46,    47,   137,    94,   155,   511,\n",
            "             2,   117,   119,     7,   121,    84,   119,   121,    84, 48836,\n",
            "            28, 45804,     7,    24,   165,    24,   135,    67,   145,   146,\n",
            "            69,   148,   121,    84,   178,     7,    46,    47,   162,   161,\n",
            "             5,   102,     1,    57,     1,   123,     2,   129,    37,     7]])\n",
            "Input: Na escola, Sofia não era a criança mais extrovertida,\n",
            "Generated: confidente cheio mais . medida crescia sentia sua reinos <UNK> uma imaginar protagonistas , histórias no protagonistas histórias no <UNK> mundo <UNK> , tinha coragem tinha eram mãos inspirou começar se suas histórias no conectava , medida crescia explorar começou e cada era dias era às uma corajosa com ,\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Referências"
      ],
      "metadata": {
        "id": "gHEJwaLlzyE-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Architeture\n",
        "https://dugas.ch/artificial_curiosity/GPT_architecture.html\n",
        "\n",
        "https://keras.io/examples/generative/text_generation_with_miniature_gpt/\n",
        "\n",
        "https://huggingface.co/learn/nlp-course/chapter7/6\n",
        "\n",
        "https://debuggercafe.com/text-generation-with-transformers/0\n",
        "\n",
        "https://debuggercafe.com/text-generation-with-transformers/\n",
        "\n",
        "\n",
        "https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf"
      ],
      "metadata": {
        "id": "fTdM4xKZzjI2"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "eoY2CAyQPeWF",
        "pNfea0_8oLZw",
        "e8FHuZBKLfMP"
      ],
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1P40xsKBfsl3KKataojOyf-0VioRb8Syf",
      "authorship_tag": "ABX9TyNuQP20wyFeagGWtfnQ4S8J",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}