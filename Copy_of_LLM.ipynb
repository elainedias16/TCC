{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elainedias16/TCC/blob/main/Copy_of_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Large language models(LLM)"
      ],
      "metadata": {
        "id": "XYA27LAQzkcY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Motivação"
      ],
      "metadata": {
        "id": "1L6pHooh0Y7T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Em 2017, a publicação do artigo \"Attention is All You Need\" revolucionou o campo do Processamento de Linguagem Natural (NLP) com a introdução dos Transformers. Essa arquitetura neural, com sua capacidade de processar sequências de dados de forma mais eficiente, impulsionou o desenvolvimento de modelos de linguagem cada vez mais sofisticados e poderosos. A partir daí, os Grandes Modelos de Linguagem (LLMs) experimentaram um vasto crescimento, atraindo investimentos consideráveis e abrindo novas fronteiras para a área. Para se manter atualizado nesse cenário, é fundamental compreender o funcionamento dos LLM, de forma a aprimorar e desenvolver novas aplicações."
      ],
      "metadata": {
        "id": "2qGtwDhi2VsL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resultados Esperados\n",
        "\n"
      ],
      "metadata": {
        "id": "THjNNlCb0Gdm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neste laboratório, espera-se que os alunos compreendam os princípios básicos do funcionamento de um Grande Modelo de Linguagem. Para exemplificar, será apresentado um código em pequena escala de um LLM."
      ],
      "metadata": {
        "id": "Ex5gvPYM5hDy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fundamentação teórica"
      ],
      "metadata": {
        "id": "xx2vUZIqz6Hm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Código"
      ],
      "metadata": {
        "id": "vIBm3uQoz4RG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código abaixo realiza a instalação e importação das bibliotecas necessárias para o desenvolvimento do modelo LLM. A principal biblioteca utilizada será o PyTorch, que fornece os módulos essenciais para redes neurais e otimização, através dos pacotes torch.nn e torch.optim, respectivamente. Também são importadas as bibliotecas Dataset e DataLoader para a criação e manipulação de datasets e mini-lotes durante o treinamento. Além disso, a biblioteca Counter, da coleção padrão, e a nltk são utilizadas para contagem e pré processamento de dados, com nltk incluindo o método word_tokenize para tokenização.\n",
        "\n",
        "\n",
        "Vale ressaltar que é possível criar um modelo LLM com outras bibliotecas, por exemplo, a Keras. A escolha da biblioteca Torch foi devido a maior familiaridade com a mesma."
      ],
      "metadata": {
        "id": "UVFoPD5HRAtw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ku2I-_TlrUyM",
        "outputId": "7cb2fdc2-c5a4-4cb7-9774-e85649a6097a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ],
      "source": [
        "!pip install transformers torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoY2CAyQPeWF"
      },
      "source": [
        "### Masked Self-Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scale dot produt attetion:\n",
        "https://paperswithcode.com/method/scaled"
      ],
      "metadata": {
        "id": "YyczlNf4nndV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RvTxKMaQUX2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import math\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "class Head(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.query = nn.Linear(config.d_model, config.head_dim, bias=config.bias)\n",
        "    self.key = nn.Linear(config.d_model, config.head_dim, bias=config.bias)\n",
        "    self.value = nn.Linear(config.d_model, config.head_dim, bias=config.bias)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    q = self.query(x)\n",
        "    k = self.key(x)\n",
        "    v = self.value(x)\n",
        "    return q, k, v\n",
        "\n",
        "\n",
        "\n",
        "class MaskedSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.num_heads = config.num_heads\n",
        "        self.head_dim = config.head_dim\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.d_model = config.d_model\n",
        "        self.heads = nn.ModuleList([Head(config) for _ in range(config.num_heads)])\n",
        "        self.output_linear = nn.Linear(config.d_model, config.d_model)\n",
        "        assert self.head_dim * self.num_heads == self.d_model, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # B, T, C = x.size()\n",
        "\n",
        "        heads_output = []\n",
        "        for head in self.heads:\n",
        "            k, q, v = head(x)\n",
        "\n",
        "\n",
        "            # Scaled dot-product attention\n",
        "            scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "\n",
        "            if mask is not None:\n",
        "                scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "            attn_weights = F.softmax(scores, dim=-1)\n",
        "            attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "            head_output = torch.matmul(attn_weights, v)\n",
        "            heads_output.append(head_output)\n",
        "\n",
        "\n",
        "        concatenated_output = torch.cat(heads_output, dim=-1)\n",
        "        output = self.output_linear(concatenated_output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NbvfI4tFlQ_u"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNfea0_8oLZw"
      },
      "source": [
        "### Feed Forward Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QyoN_gBEzPM"
      },
      "source": [
        "\n",
        "MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "dVxd-AS5ok8v"
      },
      "outputs": [],
      "source": [
        "class FeedFoward(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(config.d_model, 4 * config.d_model, bias=config.bias)\n",
        "    self.activation = nn.ReLU()\n",
        "    self.linear2 = nn.Linear(config.d_model * 4,  config.d_model, bias=config.bias)\n",
        "    self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.linear1(x)\n",
        "    x = self.activation(x)\n",
        "    x = self.linear2(x)\n",
        "    x = self.dropout(x)\n",
        "    return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8FHuZBKLfMP"
      },
      "source": [
        "### Layer Norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "hjA6NoboLhpe"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.norm = nn.LayerNorm(config.d_model, config.bias)\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.norm(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t72dRbgtM6Xy"
      },
      "source": [
        "### One Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "4RAoljJTM8Al"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.ln_1 = LayerNorm(config)\n",
        "    self.masked_self_attention = MaskedSelfAttention(config)\n",
        "    self.ln_2 = LayerNorm(config)\n",
        "    self.feed_forward = FeedFoward(config)\n",
        "\n",
        "  # def forward(self, x, mask):\n",
        "  #   x = self.ln_1(x)\n",
        "  #   x = x + self.masked_self_attention(x, mask)\n",
        "  #   x = self.ln_2(x)\n",
        "  #   x = x + self.feed_forward(x)\n",
        "  #   return x\n",
        "  def forward(self, x):\n",
        "    x = self.ln_1(x)\n",
        "    x = x + self.masked_self_attention(x)\n",
        "    x = self.ln_2(x)\n",
        "    x = x + self.feed_forward(x)\n",
        "    return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uluREArFtEP-"
      },
      "source": [
        "### Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://medium.com/@hunter-j-phillips/positional-encoding-7a93db4109e6#:~:text=class%20PositionalEncoding(nn,self.dropout(x)\n",
        "olhar posiitional"
      ],
      "metadata": {
        "id": "S9mAVQgzvNCl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código abaixo agrega os componentes criados anteriormente e acescenta os restantes para o funcionamento do modelo LLM.\n",
        "\n",
        "Assim, na inicialização da classe, tem-se o parâmetro config, que são as configurações do modelo. Em seguida, são atribuídos os blocos e a camada de normalização que foram vistas anteriormente. Além disso, são criadas as camadas de embeddings e de posição dos tokens , além da camada de dropout e uma camada linear final.\n",
        "\n",
        "\n",
        "Em seguida são criados dois métodos, o \"forward\" e o \"generate\". O método \"forward\" é responsável por definir o fluxo de dados pela rede, sendo seus parâmetros os input_ids e os targets. Os input_ids correspondem as palavras, mas quando chegam nesse ponto do código já estão em formato de tensores PyTorch. O parâmetro targets foi definido como optional (targets=None), para o caso do modelo no modo de teste, uma vez que só possível os targets no modo de treinamento.\n",
        "\n",
        "Em seguida, é definido uma variável de device que recebe o mesmo device de onde estão os tensores de entrada (palavras => tokens => input_ids => tensor) . Isso é feito para garantir que os cálculos ocorram no mesmo dispostivo.\n",
        "\n",
        "As variáveis B e T são utilizadas para armazenar o tamanho do batch e o comprimento da sequência, respectivamente. Após isso, o tensor de entrada é incorporado na camada de embeddings, de forma que cada ID no tensor passa a ter uma representação vetorial através da camada word_token_embedding. Além disso, a posição dos IDs é incorporada na camada de embeddings posicional (position_embedding), utilizando o comprimento da sequência como parâmetro para gerar a incorporação posicional. Essa incorporação posicional permite que o modelo capture a ordem dos tokens na sequência. As representações vetoriais dos tokens e suas posições são somadas (tok_emb + pos_emb) para gerar a entrada final x. Essa soma combina o significado semântico dos tokens com sua posição na sequência, o que permite ao modelo aprender tanto o contexto quanto a ordem relativa dos tokens.  Após isso, a soma (tok_emb + pos_emb) passa por uma camada de dropout para reduzir a chance de overting.\n",
        "\n",
        "\n",
        "Após a camanda de dropout, os dados passam pelos blocos que foram explicados anteriormente. Assim, os dados por seis blocos, os quais foram definidos nas configurações. Em seguida, os dados passam pela camada de normalização e pela camada linear final, a qual tem como saída os logits, que são os valores brutos da classificação gerada.\n",
        "\n",
        "No caso do modelo estar em treinamento, é adicionado uma linha para realizar o cálculo de entropia cruzada, retornando tanto os logits quanto a loss (perda).\n",
        "\n",
        "\n",
        "O método de \"generate\" é utilizado para o modo de teste do modelo. Esse método realiza um loop com o número máximo de tokens a serem gerados, chamando o método de \"forward\" em cada iteração. É criado um array de output_ids para armazenar os outputs do método de forward. Os outsputs do método forward são os logits, por causa disso, é preciso passar esses logits por uma função de softmax de forma a gerar um vetor de probabilidades do próximo token da sentença, sendo que o token de maior probabilidade é escolhido e armazenado no array output_ids. Em seguida, o token gerado é concatenado à sentença de entrada formando um novo input_ids e assim o loop continua até o máximo de número de tokens ser atingido. Por fim, o array de output_ids é retornado e precisará passar pelo método de \"decode\" para ser interpreta em linguagem natural."
      ],
      "metadata": {
        "id": "ypjg92xP4nQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# from torch.optim import Adam\n",
        "# from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.word_token_embedding = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.position_embedding = nn.Embedding(config.max_length, config.d_model)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.blocks = nn.Sequential(*[Decoder(config) for _ in range(config.n_layer)])\n",
        "        self.ln = LayerNorm(config)\n",
        "        self.ll = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, input_ids, targets=None):\n",
        "        device = input_ids.device\n",
        "        B, T = input_ids.size()\n",
        "\n",
        "        # Positional e token embed\n",
        "        tok_emb = self.word_token_embedding(input_ids)\n",
        "        pos_emb = self.position_embedding(torch.arange(T, device=device))\n",
        "        x = self.dropout(tok_emb + pos_emb)\n",
        "\n",
        "        # Transformer blocks\n",
        "        x = self.blocks(x)\n",
        "\n",
        "        # Norm layer\n",
        "        x = self.ln(x)\n",
        "\n",
        "        # Final layer\n",
        "        logits = self.ll(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            shift_logits = logits[:, :-1, :].contiguous()\n",
        "            shift_targets = targets[:, 1:].contiguous()\n",
        "            loss = F.cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_targets.view(-1))\n",
        "            return logits, loss\n",
        "\n",
        "        return logits, None\n",
        "\n",
        "    # def generate(self, input_ids, max_new_tokens):\n",
        "    #     new_tokens = []\n",
        "\n",
        "    #     for _ in range(0, max_new_tokens):\n",
        "    #         input_ids_cond = input_ids[:, -self.config.block_size:]\n",
        "    #         logits, _ = self.forward(input_ids_cond)\n",
        "    #         logits = logits[:, -1, :]\n",
        "    #         probs = F.softmax(logits, dim=-1)\n",
        "    #         input_ids_next = torch.multinomial(probs, num_samples=1)\n",
        "    #         new_tokens.append(input_ids_next)\n",
        "    #         input_ids = torch.cat((input_ids, input_ids_next), dim=1)\n",
        "\n",
        "    #     new_tokens = torch.cat(new_tokens, dim=1)\n",
        "    #     print(f\"aaaaa: {new_tokens}\")\n",
        "    #     return new_tokens\n",
        "\n",
        "\n",
        "    def generate(self, input_ids, max_new_tokens):\n",
        "      output_ids = []\n",
        "\n",
        "      for _ in range(0, max_new_tokens):\n",
        "          input_ids_cond = input_ids[:, -self.config.block_size:]\n",
        "          logits, _ = self.forward(input_ids_cond)\n",
        "          logits = logits[:, -1, :]\n",
        "          probs = F.softmax(logits, dim=-1)\n",
        "          input_ids_next = torch.multinomial(probs, num_samples=1)\n",
        "          output_ids.append(input_ids_next)\n",
        "          input_ids = torch.cat((input_ids, input_ids_next), dim=1)\n",
        "\n",
        "      output_ids = torch.cat(output_ids, dim=1)\n",
        "      print(f\"aaaaa: {output_ids}\")\n",
        "      return output_ids\n"
      ],
      "metadata": {
        "id": "ftxqAuuVC_V7"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XzNJw5itApm"
      },
      "source": [
        "### Configurações do modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código abaixo define os parâmetros gerais para o treinamento do modelo LLM. Assim, são definidos o dispositivo que será utilizado, o número máximo de tokens a serem gerados, o número de épocas de treinamento, a taxa de aprendizado do otimizador do modelo (será utilizado o otimizador Adam), o tamanho dos lotes de treinamento e o tamanho da sequência de entrada do modelo.\n",
        "\n"
      ],
      "metadata": {
        "id": "uwADM5g1KyUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "max_new_tokens = 50\n",
        "epochs = 100\n",
        "learning_rate = 0.001\n",
        "batch_size = 8\n",
        "SEQUENCE_LENGTH = 64"
      ],
      "metadata": {
        "id": "fCWqyYJYJYjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código abaixo define os hiperparâmetros que definem a arquitetura do modelo. Assim, são definidos o número de cabeças para os cálculos de atenção, a dimensão do modelo, a dimensão de cada cabeça de atenção, a taxa de dropout para evitar overfitting, a presença ou não de bias, o tamanho do vocabulário, o tamanho máximo da sequência de entrada, o número de camadas do modelo e o tamanho do bloco de entrada."
      ],
      "metadata": {
        "id": "4cdVDCYOLvw9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "collapsed": true,
        "id": "XVgYPtEGCfoY"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    num_heads = 2\n",
        "    d_model = 64\n",
        "    head_dim = 32\n",
        "    dropout = 0.1\n",
        "    bias = True\n",
        "    vocab_size = 50257      # Len tokenizer (for now)\n",
        "    # hidden_size = 1024\n",
        "    max_length = 512\n",
        "    n_layer = 6\n",
        "    block_size = SEQUENCE_LENGTH\n",
        "    # block_size = 1024\n",
        "    # block_size = 32\n",
        "\n",
        "\n",
        "config = Config()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Treinamento"
      ],
      "metadata": {
        "id": "R2UWJ5ORBnqo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código abaixo faz o download do dataset \"alice.txt\" .  Em seguida, é realiza a leitura do arquivo, utilizando a função open() para abrir o arquivo no modo de leitura ('r') e com codificação UTF-8. O conteúdo completo do arquivo é lido e armazenado na variável text."
      ],
      "metadata": {
        "id": "qiJETvi9YQQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://raw.githubusercontent.com/elainedias16/TCC/main/alice_1.txt\n",
        "\n",
        "# with open('alice_1.txt', 'r', encoding='utf-8') as f:\n",
        "#     text = f.read()"
      ],
      "metadata": {
        "id": "cU67LVvICx0t",
        "collapsed": true
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Sofia era uma menina quieta e introspectiva, mas havia algo nela que todos sabiam: seu amor pelos livros. Desde muito pequena, tinha uma curiosidade insaciável pelo mundo ao seu redor. Aos cinco anos, já estava folheando livros com ilustrações, fascinada pelas imagens e pelas poucas palavras que conhecia. À medida que crescia, essa paixão só aumentava. Seus pais a encontravam, todos os dias, escondida em algum canto da casa com um livro nas mãos, como se estivesse em outro mundo. Na escola, Sofia não era a criança mais extrovertida, mas os livros lhe davam confiança. Enquanto os colegas brincavam no pátio, ela preferia a biblioteca. A bibliotecária, Dona Clara, rapidamente se tornou sua amiga e confidente. Dona Clara sabia exatamente quais livros indicar para cada fase de Sofia. Desde contos de fadas clássicos até aventuras fantásticas, cada novo livro era uma porta para um mundo cheio de magia e descobertas. Sofia gostava de se imaginar como as protagonistas das histórias que lia. Às vezes, ela era uma exploradora destemida, em outras, uma princesa corajosa que lutava contra o mal. Mas o que mais a fascinava eram as palavras. Ela não apenas lia, mas sentia cada frase, cada parágrafo como se fosse parte da sua própria vida. Isso a inspirou a começar a escrever suas próprias histórias. No começo, suas histórias eram simples, contos sobre princesas, dragões e reinos distantes. No entanto, à medida que crescia, seus escritos se tornaram mais profundos. Ela começou a explorar temas sobre amizade, coragem e superação. Aos 12 anos, já tinha um caderno cheio de histórias que criava, e sonhava, um dia, em publicar seus próprios livros. Para Sofia, ler não era apenas um hobby; era a chave que a conectava a um mundo infinito de imaginação e conhecimento.\""
      ],
      "metadata": {
        "id": "Hm9RiHvYBdCM"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O método abaizo faz a tokenização de um texto, além de tranformar os caracteres em caracteres minúsculos."
      ],
      "metadata": {
        "id": "E_D7wwAMwbFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "  return word_tokenize(text.lower())"
      ],
      "metadata": {
        "id": "G0YnOu1Owb6F"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código abaixo cria os dicionários de codificação e decodificação das palavras, além de retornar o tamanho do vocabulário. Para criar esses dicionários, é utilizado o método \"Counter\", que contabiliza a quantidade de  cada palavra no conjunto de tokens fornecidos. Em seguida, é criada uma lista auxiliar contendo as palavras únicas do vocabulário. O dicionário \"word_to_int\" é então gerado, associando cada palavra a um índice numérico com base em sua posição no vocabulário, e o dicionário \"int_to_word\" faz o mapeamento inverso, associando os índices às palavras correspondentes."
      ],
      "metadata": {
        "id": "AjYa4qg_wpMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(tokens):\n",
        "  word_counts = Counter(tokens)\n",
        "  vocab = list(word_counts.keys())\n",
        "  word_to_int = {word: i for i, word in enumerate(vocab)}\n",
        "  int_to_word = {i: word for word, i in word_to_int.items()}\n",
        "\n",
        "  return len(vocab), word_to_int, int_to_word"
      ],
      "metadata": {
        "id": "Z9fvCotownsp"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código abaixo cria as amostras de entrada e saída para o modelo LLM. Uma vez que o dataset é apenas um conjunto de palavras, é necessário separar as palavras em data/target para que seja possível realizar o treinamento. Para isso, são passados os tokens , o dicionário de codificação de palavras e o tamanho da sequência de entrada do modelo.\n",
        "\n",
        "Inicialmente, os tokens são codificadores em IDs e são criadas amostras (samples) desses tokens. As amostras são criadas a partir da lista \"encoded\" criada anteriormente, no formato \"i : i +  sequence_length + 1\", que significa que para cada posição de i dos tokens será criada uma amostra de tamanho de sequence_length + 1, começando a sequência de tokens a partir do token i. Essas amostras são criadas em um loop de tamanho \"len(encoded) - sequence_length\", para que não \"estore\" o tamanho da lista ao acessar tokens muito perto do final, pois não é possível criar uma amostra de tamanho \"sequence_length + 1\", acessando o penúltimo token da lista, por exemplo, por isso não é possível iterar sobre toda a lista de tokens para criar amostras.\n",
        "\n",
        "\n",
        "\n",
        "Após isso, todos os tokens de cada amostra (sequence_length + 1) menos os últimos são armazenados na variável data e convertidos em um tensor, sendo que os últimos tokens de cada amostra são armazendos na variável targets e convertidos em um tensor.\n",
        "\n",
        "Esse método codifca os texto para o treinamento.\n"
      ],
      "metadata": {
        "id": "a9pkPR6ccmmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(tokens, word_to_int, sequence_length=64):\n",
        "  encoded = [word_to_int[token] for token in tokens]\n",
        "  samples = [encoded[i:i + sequence_length + 1] for i in range(len(encoded) - sequence_length)]\n",
        "  data = torch.tensor([sample[:-1] for sample in samples], dtype=torch.long)\n",
        "  targets = torch.tensor([sample[1:] for sample in samples], dtype=torch.long)\n",
        "  return data, targets"
      ],
      "metadata": {
        "id": "j-7F1jbGYJ4t"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código abaixo define a classe TextDataset, que herda da classe Dataset do PyTorch. O objetivo dessa classe é organizar o dataset em dois componentes: data e targets. Além disso, foi criado o método \"getitem\", que dado um índice, retorna uma tupla contendo o dado e o target correspondente. Também foi criado o método \"len\", que retorna o tamanho total do conjunto de dados. Posteriomente, essa classe será utilizada para instanciar um DataLoader."
      ],
      "metadata": {
        "id": "hO_BfR9sZEMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data, targets):\n",
        "        self.data = data\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.targets[idx]"
      ],
      "metadata": {
        "id": "9_XyrK4xF2h3"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O método abaixo realiza o treinamento do modelo. Ele recebe como parâmetros o modelo, o otimizador, o número de épocas de treinamento e o dataloader. Incialmente, o modelo é configurado em modo de treinamento e um loop é criado para iterar pelas épocas, com a perda total sendo inicializada em $0$. Para cada conjunto de input_ids e targets fornecido pelo dataloader, o modelo é chamado e calcula-se os logits e a perda. Em seguida, realiza-se a retropropagação da perda e a atualização dos pesos. A perda de cada lote é somada à perda total, e ao final de cada época, o valor médio da perda é exibido."
      ],
      "metadata": {
        "id": "qN3xhAGgnHAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, dataloader, optimizer, epochs):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for input_ids, targets in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            logits, loss = model(input_ids, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader)}\")"
      ],
      "metadata": {
        "id": "AOpWdVs9DYpV"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código abaixo faz a instanciação do modelo e do otimizador que será utilizadado."
      ],
      "metadata": {
        "id": "Uh2MNWDpqdaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Transformer(config)\n",
        "optimizer = Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "eFFbhwnYGzCF"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código abaixo faz a tokenizaçao do dataset e chama o método de criar o vocabulário, o qual retorna os dicionários de codificação e decodificação das palavras e o tamanho do vocabulário. Em seguida, é ajustado o tamanho do vocabulário nas configurações do modelo , pois o tamanho deixado como default  é o tamanho do tokenizador gpt-tokenizer."
      ],
      "metadata": {
        "id": "4mMzZAGGVKuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize_text = tokenize(text)\n",
        "vocab_size, word_to_int, int_to_word = build_vocab(tokenize_text)\n",
        "\n",
        "config.vocab_size = vocab_size"
      ],
      "metadata": {
        "id": "IThgzL14VMs6"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código abaixo chama o método \"prepare_data\" que separa o texto do dataset em data e targets, para que seja possível realizar o treinamento do modelo."
      ],
      "metadata": {
        "id": "jO29Vb0XVin4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data, targets = prepare_data(tokenize_text, word_to_int)"
      ],
      "metadata": {
        "id": "QoeZ6QUTVqhY"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código abaixo define um objeto do tipo TextDataset, adequado para manipular dados textuais. Em seguida, é criado um DataLoader, que permite realizar o treinamento em mini-lotes (mini-batches). O tamanho de cada lote é determinado pela variável batch_size. O parâmetro shuffle=True indica que os dados serão embaralhados a cada época de treinamento, o que contribui para a melhor generalização do modelo."
      ],
      "metadata": {
        "id": "fllYeCmaWA9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = TextDataset(data, targets)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "81mFnYm5FOlg"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código abaixa chama o método de \"train_model\" que realiza o treinamento do modelo. Para rodar o modelo sem o treinamento, basta comentar a linha abaixo."
      ],
      "metadata": {
        "id": "uN2TM_T5rENJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, dataloader, optimizer, epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-OZSxforE5m",
        "outputId": "a785f2a8-87b2-4173-e04b-5f76d3dddae0"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 6.170171439647675\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testes"
      ],
      "metadata": {
        "id": "MaAyJqd2X9kL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código abaixo realiza a codificação das palavras de um texto. Primeiro, o texto é tokenizado, e em seguida percorre-se o dicionário de mapeamento de palavras (word_to_int) para converter cada token em seu respectivo ID. Caso o token não esteja no dicionário, é utilizado o ID correspondente ao token <pad>. Por fim, a lista de IDs é convertida em um tensor do tipo long do PyTorch, que no geral é formato esperado pelos modelos LLM, quando utiliza-se a biblioteca PyTorch.\n",
        "\n",
        "Esse método é utilizado apenas para a codificação do prompt do usuário."
      ],
      "metadata": {
        "id": "pz6FwAXZFaIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(text, word_to_int):\n",
        "    tokens = tokenize(text)\n",
        "    if '<pad>' not in word_to_int:\n",
        "        pad_id = 0\n",
        "    else:\n",
        "        pad_id = word_to_int['<pad>']\n",
        "    encoded = [word_to_int.get(token, pad_id) for token in tokens]\n",
        "    return torch.tensor(encoded, dtype=torch.long).unsqueeze(0)"
      ],
      "metadata": {
        "id": "gBdiQ7KyFXLW"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código abaixo decodifica os output_ids, que são a saída do modelo criado. Os parâmetros da função são tokens (representando os output_ids) e o dicionário de decodificação int_to_word. Como os tokens podem ter dimensões adicionais, é aplicado o método squeeze() o que facilita a iteração direta sobre os tokens.\n",
        "\n",
        "Após a aplicação de squeeze(), cada token se torna um valor de dimensão única. Em seguida, o código verifica se o valor do token (obtido com token.item()) está presente no dicionário de decodificação int_to_word. Se o token estiver presente no dicionário, a palavra correspondente é adicionada à lista de palavras decodificadas. Caso contrário, o token desconhecido (<UNK>) é adicionado à lista.\n",
        "\n",
        "Após a iteração por todos os tokens, a lista de palavras decodificadas é unida em uma única string, com as palavras separadas por espaços, e essa string é retornada."
      ],
      "metadata": {
        "id": "vyUYP5QKGQKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_tokens(tokens, int_to_word):\n",
        "    decoded_words = []\n",
        "    for token in tokens.squeeze():\n",
        "        if token.item() in int_to_word:\n",
        "            decoded_words.append(int_to_word[token.item()])\n",
        "        else:\n",
        "            decoded_words.append(\"<UNK>\")\n",
        "    return ' '.join(decoded_words)"
      ],
      "metadata": {
        "id": "WhRq5T2XGLG8"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O método abaixo é função de conveniência para a geração de texto. Assim, os parâmetros são o start_text, que é o prompt do usuário, o número máximo de tokens a serem gerados e os dicionário de codificação e decodificação.\n",
        "\n",
        "Incialmente, é o modelo é colocado em modo de avaliação para desativar camadas que são úteis apenas durante o treinamento, como as camadas de dropout. Além disso, o gradiente também é desativado. Assim, é chamado o método de \"encode\" que gera um tensor, que corresponde a uma sequência de IDs que correspondem as palavras do prompt, ou sejam , as palavras que darão ínicio à geração de texto. Em seguida, é chamado o método \"generate\" do modelo, sendo o output desse método os output_ids, os quais foram chamadas de \"new_token\" no método \"generate\". Posteriormente, chamda-se o método \"decode\" que decodifica os id de saída para os tokens correspondentes.\n"
      ],
      "metadata": {
        "id": "ntdxTwemF80l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(start_text, word_to_int, int_to_word, max_new_tokens=50):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        input_ids = encode(start_text, word_to_int) #inputs_ids no formato de tensores\n",
        "        generated_tokens = model.generate(input_ids, max_new_tokens)\n",
        "        return decode_tokens(generated_tokens, int_to_word)"
      ],
      "metadata": {
        "id": "3l0D1tiTF5ms"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código abaixo realiza um conjunto de testes para o modelo criado. Assim, é definido uma sequência de sentenças para servirem de start_text para o modelo. Em seguida, percorre-se cada uma das senteças, chamando o método \"generate_text\" para gerar o texto de saída."
      ],
      "metadata": {
        "id": "qn5Ak3UOp1t9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentences = [\n",
        "    \"Sofia era uma menina quieta\",\n",
        "    \"Na escola, Sofia não era a criança mais extrovertida,\",\n",
        "]\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    # generated_text = generate_text(model, sentence, word_to_int, int_to_word, max_new_tokens=50)\n",
        "    generated_text = generate_text(sentence, word_to_int, int_to_word, max_new_tokens=50)\n",
        "    print(f\"Input: {sentence}\")\n",
        "    print(f\"Generated: {generated_text}\")\n",
        "    print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyroKBFmFZvS",
        "outputId": "81f2857b-6f2c-4214-84d1-4dae660cf78e"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aaaaa: tensor([[   15,    37,    91,    48,    33,    19,   158,   159,   159, 47135,\n",
            "             7,   165,   104,    99,    83,   102,   104,    12,     7,    69,\n",
            "           122,    20,   124,    24,   108,   104,    20,    53, 14453, 40589,\n",
            "           168, 45275,   101,   103,    92,     8,    37,   156,   104,    31,\n",
            "             7, 32598,   155,    69,   135,   142,   107,    83,     7,   130]])\n",
            "Input: Sofia era uma menina quieta\n",
            "Generated: : com clara essa anos livros escritos tornaram tornaram <UNK> , coragem de quais brincavam cada de que , se lia . vezes tinha até de . pais <UNK> <UNK> caderno <UNK> para fase rapidamente mas com distantes de aos , <UNK> reinos se eram própria clássicos brincavam , lutava\n",
            "================================================================================\n",
            "aaaaa: tensor([[170,  57,   7, 121, 169,   7, 121, 136,  44,  28,  86,  54,  12,  74,\n",
            "         136,   7, 128,  46,  47, 128,  52,  54,   7, 117, 118,  20,  52,  69,\n",
            "         141, 108,  56,  92,  95,  56,  61, 104, 105, 106,  38,  47,  48,  74,\n",
            "          86,  54,  54,   7,  69, 141, 132,  98]])\n",
            "Input: Na escola, Sofia não era a criança mais extrovertida,\n",
            "Generated: sonhava dias , histórias criava , histórias apenas conhecia mundo ela a que não apenas , princesa medida crescia princesa seus a , imaginar as . seus se parte até os rapidamente amiga os canto de contos fadas ilustrações crescia essa não ela a a , se parte o exatamente\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Referências"
      ],
      "metadata": {
        "id": "gHEJwaLlzyE-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Architeture\n",
        "https://dugas.ch/artificial_curiosity/GPT_architecture.html\n",
        "\n",
        "https://keras.io/examples/generative/text_generation_with_miniature_gpt/\n",
        "\n",
        "https://huggingface.co/learn/nlp-course/chapter7/6\n",
        "\n",
        "https://debuggercafe.com/text-generation-with-transformers/0\n",
        "\n",
        "https://debuggercafe.com/text-generation-with-transformers/\n",
        "\n",
        "\n",
        "https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf"
      ],
      "metadata": {
        "id": "fTdM4xKZzjI2"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "eoY2CAyQPeWF",
        "pNfea0_8oLZw",
        "e8FHuZBKLfMP"
      ],
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1P40xsKBfsl3KKataojOyf-0VioRb8Syf",
      "authorship_tag": "ABX9TyM0UanYs1TNdIfxkSuIrYcw",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}